{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52af13c7",
   "metadata": {},
   "source": [
    "# **Implementing a Quantum-Classical Parameter Mapping**\n",
    "# **NO TESTS OR EXPLANATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0f42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "import math\n",
    "import re\n",
    "from numba import njit\n",
    "from itertools import product\n",
    "from scipy.linalg import kron\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "from numba.typed import List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b1dc355",
   "metadata": {},
   "source": [
    "## **Obtaining a valid transition matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1411bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  CONVERTING SPIN <-> BINARY FUNCTIONS\n",
    "#----------------------------------------------------------------------\n",
    "@njit\n",
    "def index_to_spin_state(s_idx: int, N: int) -> np.ndarray:\n",
    "    \"\"\"Convert an index to a spin state.\"\"\"\n",
    "    s = np.zeros((N,), dtype=np.int64)\n",
    "    for i in range(N):\n",
    "        # get the ith bit of s_idx.\n",
    "        bit = (s_idx >> i) & 1\n",
    "        # convert the bit to a spin (-1 or 1).\n",
    "        s[N - 1 - i] = bit * 2 - 1\n",
    "    return s\n",
    "\n",
    "@njit\n",
    "def spin_state_to_index(s: np.ndarray) -> int:\n",
    "    \"\"\"Convert a spin state to an index.\"\"\"\n",
    "    N = len(s)\n",
    "    s_idx = 0\n",
    "    for i in range(N):\n",
    "        # convert the spin to a bit (0 or 1).\n",
    "        bit = (s[i] + 1) // 2\n",
    "        # set the ith bit of s_idx.\n",
    "        s_idx |= bit << (N - 1 - i)\n",
    "    return s_idx\n",
    "\n",
    "@njit\n",
    "def bits_flipped_indices(s_idx: int, flip_index: int, N: int) -> List[int]:\n",
    "    \"\"\"Return the indices of the bits that are flipped when going from s_idx to flip_index.\"\"\"\n",
    "    flipped_bits = s_idx ^ flip_index\n",
    "    flipped_indices = []\n",
    "    for k in range(N):\n",
    "        if (flipped_bits & (1 << k)) != 0:\n",
    "            flipped_indices.append(N - 1 - k)\n",
    "    return flipped_indices\n",
    "\n",
    "@njit\n",
    "def steady_state(A):\n",
    "    \"\"\"Calculates the steady state of a (left) stochastic matrix\"\"\"\n",
    "    # compute eigenvalues and corresponding eigenvectors for the transposed matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(A.T)\n",
    "    \n",
    "    # get the eigenvector corresponding to eigenvalue 1 (the steady state)\n",
    "    steady_state = np.abs(np.real(eigenvectors[:, np.argmax(np.real(eigenvalues))]))\n",
    "    \n",
    "    # normalize the vector to get a probability distribution\n",
    "    steady_state = steady_state / np.sum(steady_state)\n",
    "    \n",
    "    return steady_state.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d015206",
   "metadata": {},
   "outputs": [],
   "source": [
    "### AUXILARY SIMULATION FUNCTIONS\n",
    "#----------------------------------------------------------------------\n",
    "@njit\n",
    "def g_single(i: int, s: np.ndarray, w: np.ndarray, h: np.ndarray, J: np.ndarray):\n",
    "    \"\"\"Calculates the single flip probability.\"\"\"\n",
    "    sum_Js = np.sum(J[i, :] * s)\n",
    "    return np.exp(-w[i, i] - s[i] * (h[i] + sum_Js))\n",
    "\n",
    "@njit\n",
    "def g_double(i: int, j: int, s: np.ndarray, w: np.ndarray, h: np.ndarray, J: np.ndarray):\n",
    "    \"\"\"Calculates the double flip probability\"\"\"\n",
    "    sum_Js_i = np.sum(J[i, :] * s) - J[i, j] * s[j]\n",
    "    sum_Js_j = np.sum(J[j, :] * s) - J[i, j] * s[i]\n",
    "\n",
    "    return np.exp(-w[i, j] + J[i, j] * s[i] * s[j] - s[i] * (sum_Js_i + h[i]) - s[j] * (sum_Js_j + h[j]))\n",
    "\n",
    "@njit\n",
    "def choice(probabilities: np.ndarray) -> int:\n",
    "    \"\"\"A workaround for np.random.choice, which is unsupported by numba\"\"\"\n",
    "    cumulative_distribution = np.cumsum(probabilities)\n",
    "    return np.searchsorted(cumulative_distribution, np.random.random(), side =\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "152fff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def compute_transition_matrix(w: np.ndarray, h: np.ndarray, J: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Computes the transition matrix for the given parameters.\"\"\"\n",
    "    N = len(h)  # number of spins in the lattice\n",
    "    transition_matrix = np.zeros((2**N, 2**N))\n",
    "\n",
    "    #sum over rows to fill\n",
    "    for s_idx in range(2**N):\n",
    "        s = index_to_spin_state(s_idx, N)\n",
    "        flip_probs = np.zeros((2**N))  # initialize with zeros\n",
    "\n",
    "        for i in range(N):\n",
    "            flip_index = s_idx ^ (1 << i)\n",
    "            i_flipped = bits_flipped_indices(s_idx, flip_index, N)[0]\n",
    "            flip_probs[flip_index] = g_single(i_flipped, s, w, h, J)  # single flip probabilities\n",
    "\n",
    "        for i in range(N):\n",
    "            for j in range(i+1, N):\n",
    "                flip_index = s_idx ^ (1 << i) ^ (1 << j)\n",
    "                i_flipped, j_flipped = bits_flipped_indices(s_idx, flip_index, N)\n",
    "                flip_probs[flip_index] = g_double(i_flipped, j_flipped, s, w, h, J)  # double flip probabilities\n",
    "\n",
    "       # add the no-flip probability and store the flipping probabilities\n",
    "        no_flip_prob = 1 - sum(flip_probs)\n",
    "        flip_probs[s_idx] = no_flip_prob\n",
    "        transition_matrix[s_idx] = flip_probs\n",
    "\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cffbc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_w(w, h, J, step_size = 1e-4, no_flip_prob = 0):\n",
    "    \"\"\"Checks if w gives rise to a proper normalized W. Prints the minimal value of w required if it's not the case\"\"\"\n",
    "    min_scalar = 0\n",
    "    W = compute_transition_matrix(w, h, J)\n",
    "    self_probs = np.diag(W.copy())\n",
    "\n",
    "    if np.all(self_probs >= no_flip_prob):\n",
    "        return\n",
    "    else:\n",
    "        self_probs_old = self_probs.copy()\n",
    "        while True:\n",
    "            new_w             = w + np.ones(w.shape) * min_scalar\n",
    "            W = compute_transition_matrix(new_w, h, J)\n",
    "            self_probs        = np.diag(W)\n",
    "\n",
    "            # check if the self-transition probabilities are all at least no_flip_prob\n",
    "            if np.all(self_probs >= no_flip_prob):\n",
    "                break\n",
    "\n",
    "            # if the condition is not met, increase the scalar value and try again\n",
    "            min_scalar += step_size\n",
    "\n",
    "        assert np.all(self_probs_old >= no_flip_prob), f\"Need to use larger w matrix: \\n {new_w} \"\n",
    "\n",
    "def check_parameters(w, J, h):\n",
    "    \"\"\"Check if parameters are chosen to give rise to a normalized W that satisfies detailed balance\"\"\"\n",
    "    check_w(w, h, J)\n",
    "    assert np.allclose(J, J.T) and np.allclose(w, w.T) and np.all(J.diagonal() == 0),  \"Input matrices w and J should be symmetric and J should have a zero diagonal\"\n",
    "\n",
    "\n",
    "def check_detailed_balance(W):\n",
    "    \"\"\"\n",
    "    Check if the transition matrix W and stationary distribution pi satisfy the detailed balance condition.\n",
    "    \"\"\"\n",
    "    pi = steady_state(W)\n",
    "    n_states = W.shape[0]\n",
    "    for i in range(n_states):\n",
    "        for j in range(n_states):\n",
    "            assert np.isclose(W[i, j] * pi[i], W[j, i] * pi[j]), \"Transition matrix does not satisfy detailed balance\"\n",
    "\n",
    "\n",
    "def test_transition_matrix(W):\n",
    "    \"\"\"Test that the sum of the probabilities in each row of the transition matrix is 1 and there are no negative elements.\"\"\"\n",
    "    for row in W:\n",
    "        assert np.isclose(np.sum(row), 1), \"Row sums to \" + str(np.sum(row))\n",
    "        assert np.all(row >= 0), \"Negative probability in row \" + str(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67c00f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  SIMULATION FUNCTIONS\n",
    "#----------------------------------------------------------------------\n",
    "@njit\n",
    "def spin_flip(s: np.ndarray, transition_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Flipping algorithm, uses precomputed transition matrix. \"\"\"\n",
    "    N = len(s)  # number of spins in the lattice\n",
    "\n",
    "    # calculate the index of the current state in the transition matrix\n",
    "    s_idx = spin_state_to_index(s)\n",
    "\n",
    "    # get probabilities from transition matrix\n",
    "    flip_probs = transition_matrix[s_idx]\n",
    "\n",
    "    # randomly choose a spin flip according to its probability of occurring\n",
    "    idx = choice(flip_probs)\n",
    "\n",
    "    # apply the chosen spin flip\n",
    "    flipped_indices = bits_flipped_indices(s_idx, idx, N)\n",
    "    \n",
    "    if len(flipped_indices)   == 0:  # no flip\n",
    "        pass\n",
    "    elif len(flipped_indices) == 1:  # single flip\n",
    "        i = flipped_indices[0]\n",
    "        s[i] *= -1\n",
    "    elif len(flipped_indices) == 2:  # double flip\n",
    "        i, j = flipped_indices\n",
    "        s[i] *= -1\n",
    "        s[j] *= -1\n",
    "\n",
    "    return s\n",
    "\n",
    "@njit\n",
    "def simulate_dynamics(W: np.ndarray, steps: int, N: int) -> np.ndarray:\n",
    "    \"\"\"Simulates the dynamics of the system for the given number of steps.\"\"\"\n",
    "    s = np.array([np.random.choice(np.array([-1, 1])) for _ in range(N)], dtype=np.int64) #initial config\n",
    "    trajectory = np.empty((steps, N), dtype=np.int64)\n",
    "\n",
    "    for t in range(steps):\n",
    "        s = spin_flip(s, W)\n",
    "        trajectory[t] = s\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c67b2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def infer_transition_matrix(trajectory: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Infers the transition matrix from a time series of the states of the system\"\"\"\n",
    "    N = trajectory.shape[1]  # number of spins in the lattice\n",
    "    steps = trajectory.shape[0]  # number of steps in the trajectory\n",
    "    transition_matrix = np.zeros((2**N, 2**N))\n",
    "\n",
    "    for t in range(steps - 1):\n",
    "        # convert the spin states to indices\n",
    "        s_idx = spin_state_to_index(trajectory[t])\n",
    "        next_s_idx = spin_state_to_index(trajectory[t + 1])\n",
    "\n",
    "        # increment the corresponding cell in the transition matrix\n",
    "        transition_matrix[s_idx, next_s_idx] += 1\n",
    "\n",
    "    # normalize each row to get probabilities\n",
    "    for i in range(2**N):\n",
    "        row_sum = np.sum(transition_matrix[i])\n",
    "        if row_sum > 0:  # avoid division by zero\n",
    "            transition_matrix[i] /= row_sum\n",
    "\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d04c6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  PLOTTING FUNCTIONS\n",
    "#----------------------------------------------------------------------\n",
    "def plot_combined_dynamics(trajectory, N):\n",
    "    fig, (ax1,ax2)= plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot for the system's evolution through states\n",
    "    states = [(''.join(['0' if spin == -1 else '1' for spin in s])) for s in trajectory]  # convert spins to binary\n",
    "    unique_states = sorted(set(states))\n",
    "    state_indices = [unique_states.index(state) for state in states]\n",
    "    \n",
    "    ax1.step(range(len(state_indices)), state_indices)\n",
    "    ax1.set_yticks(range(len(unique_states)))\n",
    "    ax1.set_yticklabels(unique_states)\n",
    "    ax1.set_title(\"System's evolution through states\")\n",
    "    ax1.set_xlabel('Time step')\n",
    "    ax1.set_ylabel('State')\n",
    "\n",
    "    # Plot for the spins' evolution\n",
    "    offset_labels = []\n",
    "    y_ticks = []\n",
    "    for i in range(N):\n",
    "        spin_trajectory = trajectory[:, i] + i * 3\n",
    "        ax2.step(range(len(spin_trajectory)), spin_trajectory, label=f'Spin {i+1}')\n",
    "        offset_labels += ['0', '1']\n",
    "        y_ticks += [(i*3 - 1), (i*3 +1)]\n",
    "        \n",
    "    ax2.set_title(\"Spins' evolution\")\n",
    "    ax2.set_xlabel('Time step')\n",
    "    ax2.set_ylabel('Spin value (with offset)')\n",
    "    ax2.set_yticks(y_ticks)\n",
    "    ax2.set_yticklabels(offset_labels)\n",
    "    ax2.legend(loc='best')\n",
    "    ax2.plot()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bb52bb6",
   "metadata": {},
   "source": [
    "## **Implementation of the Boltzmann Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f95c7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  BOLTZMANN MACHINE AUXILARY FUNCTIONS\n",
    "#----------------------------------------------------------------------\n",
    "def all_possible_configs(N: int) -> np.ndarray:\n",
    "    '''Generates a 2^N by N matrix with all possible configurations of a binary spin system of size N'''\n",
    "    configs = np.zeros((2**N,N))\n",
    "    for i in range(2**N):\n",
    "        config = np.array([1 if x == '1' else -1 for x in np.binary_repr(i, width=N)])\n",
    "        configs[i] = config\n",
    "    return configs\n",
    "\n",
    "@njit\n",
    "def calcEnergy(config: np.ndarray, J: np.ndarray, h: np.ndarray) -> float:\n",
    "    '''Calculates the energy of a given configuration'''\n",
    "    config_float = config.astype(np.float64)  # convert config to float\n",
    "    energy = -np.dot(config_float.T, np.dot(J, config_float)) / 2 - np.dot(h, config_float)\n",
    "    return energy\n",
    "\n",
    "@njit\n",
    "def calcMag(config: np.ndarray) -> float:\n",
    "    ''' Magnetizations of a given configuration '''\n",
    "    return np.sum(config)\n",
    "\n",
    "@njit\n",
    "def calcCorr(config: np.ndarray) -> np.ndarray: \n",
    "    '''Calculates the spin-spin correlations of a given configuration. Returns a NxN matrix with the correlations'''\n",
    "    return np.outer(config, config.T)\n",
    "\n",
    "@njit\n",
    "def log_likelihood(p: np.ndarray, J: np.ndarray, h: np.ndarray, configs: np.ndarray) -> float:\n",
    "    \"\"\"Calculates the log-likelihood of the system under the Boltzmann distribution.\"\"\"\n",
    "    Z = 0\n",
    "    for config in configs:\n",
    "        Ene = calcEnergy(config, J, h)\n",
    "        Z += np.exp(-Ene)\n",
    "    logZ = np.log(Z)\n",
    "\n",
    "    log_likelihood = 0\n",
    "    for i, config in enumerate(configs):\n",
    "        Ene = calcEnergy(config, J, h)\n",
    "        log_likelihood += p[i] * (-Ene - logZ)\n",
    "\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfbd80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  BOLTZMANN MACHINE FUNCTIONS\n",
    "#----------------------------------------------------------------------\n",
    "@njit\n",
    "def ising_solve_exact_simplified(N: int, J: np.ndarray, h: np.ndarray, configs: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:    \n",
    "    '''Calculates the free statistics for a single temperature using exact calculation'''\n",
    "    beta = 1.0\n",
    "    Z, C1, C2  = 0, np.zeros((N)), np.zeros((N,N))    #  initialize arrays to store variables                       \n",
    "                                           \n",
    "    for config in configs:\n",
    "        Ene  = calcEnergy(config,J,h)\n",
    "        Corr = calcCorr(config) \n",
    "        p    = np.exp(-beta * Ene)                    #  non-normalized probability of this  configuration at this temperature\n",
    "        Z  +=  p                                        \n",
    "        C1 +=  p * config                               \n",
    "        C2 +=  p * Corr\n",
    "\n",
    "    Ci   = C1 / Z\n",
    "    CiCj = C2 / Z\n",
    "        \n",
    "    return Ci,CiCj\n",
    "\n",
    "@njit\n",
    "def classical_boltzmann_machine(p: np.ndarray, N: int, configs: np.ndarray, maxiter: int, learning_rate: float, tol: float = 1e-14) -> Tuple[np.ndarray, np.ndarray, List[np.ndarray], List[np.ndarray], List[float]]:\n",
    "    '''Solves the inverse Ising problem. Returns weight matrices that generated the clamped statistics \n",
    "    and the inferred weight matrix'''\n",
    "    J_assym = 2.0 * np.random.random((N, N)) - 1.0   # random initial values between -1 and 1\n",
    "    J = (J_assym+ J_assym.T)/2                       # symmetrize the random matrix \n",
    "    np.fill_diagonal(J, 0)                           # set diagonal of J to 0 to ensure detailed balance\n",
    "    h = 2.0 * np.random.random(N) - 1.0              # random initial values between -1 and 1\n",
    "\n",
    "    # compute the expected spin values\n",
    "    Ci_clamp = np.dot(p, configs)\n",
    "\n",
    "    # compute the expected spin-spin correlations\n",
    "    CiCj_clamp = configs.T @ np.diag(p) @ configs\n",
    "        \n",
    "    #  initialize gradient ascent values  \n",
    "    it    = 0\n",
    "    delta_J = np.inf\n",
    "    delta_h = np.inf\n",
    "    # old_log_likelihood = -np.inf\n",
    "\n",
    "    # initialize lists to store the values of the parameters and the log-likelihood at each iteration\n",
    "    J_values = []\n",
    "    h_values = []\n",
    "    log_likelihood_values = []\n",
    "    \n",
    "    while (it < maxiter and (delta_J > tol or delta_h > tol)):    #  start gradient ascent\n",
    "        it += 1\n",
    "\n",
    "        #generate free statistics based on type of solver\n",
    "        Ci_free, CiCj_free = ising_solve_exact_simplified(N, J, h, configs) \n",
    "\n",
    "        # compute the updates for J and h\n",
    "        delta_J = learning_rate * (CiCj_clamp - CiCj_free)\n",
    "        delta_h = learning_rate * (Ci_clamp   - Ci_free)\n",
    "            \n",
    "        h     = h + delta_h               #  update weight matrix based on statistics  \n",
    "        J     = J + delta_J\n",
    "        \n",
    "        # compute the magnitude of the updates\n",
    "        delta_J = np.linalg.norm(delta_J)\n",
    "        delta_h = np.linalg.norm(delta_h)\n",
    "\n",
    "        # calculate the log-likelihood and check for convergence\n",
    "        new_log_likelihood = log_likelihood(p, J, h, configs)\n",
    "        # if np.abs(new_log_likelihood - old_log_likelihood) < tol:\n",
    "        #     break\n",
    "        # old_log_likelihood = new_log_elikelihood\n",
    "\n",
    "        # store the current values of the parameters and the log-likelihood\n",
    "        J_values.append(J)\n",
    "        h_values.append(h)\n",
    "        log_likelihood_values.append(new_log_likelihood)\n",
    "        \n",
    "    return J, h, J_values, h_values, log_likelihood_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40127f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  PLOTTING FUNCTIONS\n",
    "#----------------------------------------------------------------------\n",
    "def plot_scatter(ax, x, y, xlabel, ylabel, color, size=10):\n",
    "    '''Creates a scatter subplot'''\n",
    "    ax.scatter(x, y, s=size, marker='o', color=color)\n",
    "    ax.set_xlabel(xlabel, fontsize=20)\n",
    "    ax.set_ylabel(ylabel, fontsize=20)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "def plot_convergence(J_values, h_values, log_likelihood_values, title='Convergence Plots', size=10):\n",
    "    '''Plots the convergence of the parameters and the log-likelihood.'''\n",
    "    it = len(J_values)\n",
    "    its = np.arange(1, it+1, 1)\n",
    "    its_diff = np.arange(1, it, 1)\n",
    "\n",
    "    J_diff = np.diff(J_values, axis=0)\n",
    "    h_diff = np.diff(h_values, axis=0)\n",
    "\n",
    "    J_max = np.linalg.norm(J_diff, axis=(1,2))\n",
    "    h_max = np.linalg.norm(h_diff, axis=1)\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 6));    #  make plots\n",
    "    fig.suptitle(title, fontsize=30, y = 1)\n",
    "    trans = mtransforms.ScaledTranslation(-20/72, 7/72, fig.dpi_scale_trans)\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    plot_scatter(ax1, its_diff , J_max, \"Iterations\", r\"$ (\\Delta h)_{max}$\", \"IndianRed\", size)\n",
    "    ax1.text(0, 1.0, 'A.)', transform=ax1.transAxes + trans, fontsize='large',fontweight ='bold', va='bottom', fontfamily='sans-serif')\n",
    "    \n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    plot_scatter(ax2, its_diff , h_max, \"Iterations\", r\"$ (\\Delta J)_{max}$ \", \"SteelBlue\", size)\n",
    "    ax2.text(0, 1.0, 'B.)', transform=ax2.transAxes + trans, fontsize='large',fontweight ='bold', va='bottom', fontfamily='sans-serif')\n",
    "    \n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    ax3.scatter(its, log_likelihood_values, s=size, marker='o', color=\"Coral\")\n",
    "    ax3.set_xlabel(\"Iterations\", fontsize=20)\n",
    "    ax3.set_ylabel(\"Log Likelihood\", fontsize=20)\n",
    "    ax3.text(0, 1.0, 'C.)', transform=ax3.transAxes + trans, fontsize='large',fontweight ='bold', va='bottom', fontfamily='sans-serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc8e4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_w(W: np.ndarray, J: np.ndarray, h: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Computes the parameters w from the flip probabilities, J, h, and the spin states.\"\"\"\n",
    "    # Initialize w with zeros\n",
    "    N = len(h)\n",
    "    w = np.zeros((N, N))\n",
    "    from_idx = 0 # arbitarly choose first row of W\n",
    "    s = index_to_spin_state(from_idx, N)\n",
    "\n",
    "    # sum over row entries of W, to find all possible w\n",
    "    for to_idx in range(1,2**N,1):\n",
    "        # calculate w_ii\n",
    "        flipped_indices = bits_flipped_indices(from_idx, to_idx, N)\n",
    "        \n",
    "        if len(flipped_indices) == 0:  # no flip\n",
    "            pass\n",
    "\n",
    "        elif len(flipped_indices) == 1:  # single flip, compute wii\n",
    "            i = flipped_indices[0]\n",
    "            sum_Js = np.sum(J[i, :] * s)\n",
    "            w[i, i] = -np.log(W[from_idx,to_idx] + 1e-10) - s[i] * (h[i] + sum_Js)\n",
    "\n",
    "        elif len(flipped_indices) == 2:  # double flip, compute wij\n",
    "            j, i = flipped_indices\n",
    "            sum_Js_i = np.sum(J[i, :] * s) - J[i, j] * s[j]\n",
    "            sum_Js_j = np.sum(J[j, :] * s) - J[i, j] * s[i]\n",
    "            w[i, j] = -np.log(W[from_idx,to_idx]+ 1e-10) + J[i, j] * s[i] * s[j] - s[i] * (sum_Js_i + h[i]) - s[j] * (sum_Js_j + h[j])\n",
    "            w[j, i] = w[i, j]  # w is symmetric\n",
    "\n",
    "    return w\n",
    "\n",
    "def infer_parameters(W: np.ndarray, maxiter: int = 2**20, learning_rate: float = 0.1, tolerance = 1e-10, plot = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Infers the parameters w, h, J from a given transition matrix.\"\"\"\n",
    "    N = int(np.log2(W.shape[0]))\n",
    "\n",
    "    # infer the equilibrium distribution p from W\n",
    "    p = steady_state(W)\n",
    "\n",
    "    # run the Boltzmann machine\n",
    "    configs = all_possible_configs(N)\n",
    "    J, h, J_values, h_values, log_likelihood_values = classical_boltzmann_machine(p, N, configs, maxiter, learning_rate, tolerance )\n",
    "\n",
    "    w = compute_w(W, J, h)\n",
    "\n",
    "    if plot == True:\n",
    "        plot_convergence(J_values, h_values, log_likelihood_values, title = 'BM Convergence')\n",
    "\n",
    "    return w, J, h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28df998d",
   "metadata": {},
   "source": [
    "## **Obtaining a valid density matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9f2b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Pauli matrices globally\n",
    "PAULI_I = np.array([[1, 0], [0, 1]],    dtype=np.complex128)\n",
    "PAULI_X = np.array([[0, 1], [1, 0]],    dtype=np.complex128)\n",
    "PAULI_Y = np.array([[0, -1j], [1j, 0]], dtype=np.complex128)\n",
    "PAULI_Z = np.array([[1, 0], [0, -1]],   dtype=np.complex128)\n",
    "\n",
    "# create an array of Pauli matrices and their labels\n",
    "PAULI_MATRICES = np.array([PAULI_I, PAULI_X, PAULI_Y, PAULI_Z], dtype=np.complex128)\n",
    "PAULI_LABELS = ['I', 'X', 'Y', 'Z']\n",
    "\n",
    "def tensor_product(matrices: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Compute the tensor product of a list of matrices.\"\"\"\n",
    "    result = matrices[0]\n",
    "    # iterate over each matrix in the list starting from the second one\n",
    "    for matrix in matrices[1:]:\n",
    "        result = kron(result, matrix)\n",
    "    return result\n",
    " \n",
    "def generate_interaction_matrices(num_qubits: int) -> Tuple[np.ndarray, np.ndarray, Dict[Tuple[str, ...], Optional[float]]]:\n",
    "    \"\"\"Constructs the tensor product of Pauli matrices for each qubit pair.\"\"\"\n",
    "    num_combinations = 4 ** num_qubits # total number of combinations of Pauli matrices for num_qubits\n",
    "    interaction_matrices = np.empty((num_combinations, 2 ** num_qubits, 2 ** num_qubits), dtype=np.complex128) \n",
    "    interaction_labels = np.empty((num_combinations, num_qubits), dtype=object)\n",
    "\n",
    "    # initialize an empty dictionary to store the weights associated with each interaction matrix\n",
    "    interaction_weights = {}\n",
    "\n",
    "    # enumerate over all possible combinations of Pauli matrices for num_qubits\n",
    "    for idx, matrix_indices in enumerate(product(range(4), repeat=num_qubits)):\n",
    "        # select the corresponding Pauli matrices for the current combination\n",
    "        matrices = PAULI_MATRICES[list(matrix_indices)]\n",
    "        # compute the tensor product of the selected matrices\n",
    "        interaction_matrix = tensor_product(matrices)\n",
    "\n",
    "        # store the computed tensor product matrix and its corresponding labe\n",
    "        interaction_matrices[idx] = interaction_matrix\n",
    "        label_tuple = tuple(PAULI_LABELS[i] for i in matrix_indices)\n",
    "        interaction_labels[idx] = label_tuple\n",
    "\n",
    "        # initialize the weight associated with this interaction matrix\n",
    "        # by default, it is set to None. You can change it later.\n",
    "        interaction_weights[label_tuple] = None\n",
    "\n",
    "    return interaction_matrices, interaction_labels, interaction_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69a88de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_dict_to_array(interaction_labels: np.ndarray, interaction_weights: Dict[Tuple[str, ...], Optional[float]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts the interaction_weights dictionary to a numpy array based on the order of interaction_labels.\n",
    "    \"\"\"\n",
    "    weights_array = np.array([interaction_weights[tuple(label)] if interaction_weights[tuple(label)] is not None else 0 for label in interaction_labels])\n",
    "    return weights_array\n",
    "\n",
    "def weights_array_to_dict(interaction_labels: np.ndarray, weights_array: np.ndarray) -> Dict[Tuple[str, ...], float]:\n",
    "    \"\"\"\n",
    "    Converts a numpy array to an interaction_weights dictionary based on the order of interaction_labels.\n",
    "    \"\"\"\n",
    "    interaction_weights = {tuple(label): weights_array[i] for i, label in enumerate(interaction_labels)}\n",
    "    return interaction_weights\n",
    "\n",
    "def generate_random_parameter_matrix(random_seed: int, num_qubits: int, interaction_labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a random parameter array for the random seed value random_seed.\n",
    "    Interactions containing more than 2-body interactions are set to zero.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)  # set the random seed value\n",
    "    num_params = 4**num_qubits   # calculate the number of parameters for the weight matrix\n",
    "    w_array = np.random.rand(num_params)  # generate a random parameter array of size num_params\n",
    "\n",
    "    # iterate over the interaction labels\n",
    "    for i, label_tuple in enumerate(interaction_labels):\n",
    "        # count the number of 'I' in the current label tuple\n",
    "        num_I = np.count_nonzero(label_tuple == 'I')\n",
    "        # if there are more than 2 'I', set the corresponding weight to 0\n",
    "        if num_I < num_qubits - 2:\n",
    "            w_array[i] = 0\n",
    "\n",
    "    w_array[0] = 0 # -log(Z) = 0\n",
    "    return w_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03e605e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def hamiltonian_n_qubits(interaction_weights: np.ndarray, interaction_matrices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the Hamiltonian matrix of an n-qubit system\n",
    "    \"\"\"\n",
    "    num_qubits = int(np.log2(interaction_matrices.shape[1]))\n",
    "    H = np.zeros((2**num_qubits, 2**num_qubits), dtype=np.complex128)\n",
    "    for i in range(len(interaction_weights)):\n",
    "        H += interaction_weights[i] * interaction_matrices[i]\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54396d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def expmat(A: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the exponential of a given matrix `A'.\n",
    "    \"\"\"\n",
    "    A = 0.5 * (A + np.transpose(np.conjugate(A)))\n",
    "    evals, evecs = np.linalg.eigh(A)\n",
    "    N = len(evals)\n",
    "    res = np.zeros((N,N),np.complex128)\n",
    "    for i in range(N):\n",
    "        eigenvector = evecs[:,i]\n",
    "        projector = np.outer(eigenvector,eigenvector.conj())\n",
    "        res += np.exp(evals[i]) * projector\n",
    "    return res\n",
    "\n",
    "@njit\n",
    "def logmat(A: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the natural logarithm of a given matrix `A`. Same structure as expmat.\n",
    "    \"\"\"\n",
    "    A = 0.5 * (A + np.transpose(np.conjugate(A)))\n",
    "    evals, evecs = np.linalg.eigh(A)\n",
    "    N = len(evals)\n",
    "    res = np.zeros((N,N),np.complex128)\n",
    "    for i in range(N):\n",
    "        eigenvector = evecs[:,i]\n",
    "        projector = np.outer(eigenvector,eigenvector.conj())\n",
    "        res += np.log(evals[i]) * projector\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac6ccf34",
   "metadata": {},
   "source": [
    "Having this, we only have to calculate $ \\rho = \\frac{e^{\\hat{H}}}{Z} $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "389015e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def rho_model(interaction_weights: np.ndarray, interaction_matrices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the density matrix of an n-qubit system using the Hamiltonian\n",
    "    and the interaction matrices, and then normalizes it. Exact Diagonalization.\n",
    "    \"\"\"\n",
    "    H = hamiltonian_n_qubits(interaction_weights, interaction_matrices)  # get Hamiltonian matrix\n",
    "    rho = expmat(H)                                                      # definition of rho\n",
    "\n",
    "    Z = np.real(np.trace(rho))                                           # get Z\n",
    "    rho /= Z                                                             # normalize such that Tr[rho] = 1\n",
    "    # for errors in numerical precision\n",
    "    rho += 1e-6 * np.eye(rho.shape[0])\n",
    "\n",
    "    return rho \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29064da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_density_matrix(rho, check_negative = False):  \n",
    "    \"\"\"Checks if the trace of rho is equal to 1 and if its eigenvalues are positive-semidefinite. Optional check for negative rho entries\"\"\"\n",
    "\n",
    "    eigenvalues, _ = np.linalg.eig(rho)\n",
    "    # check if rho is positive semi-definite\n",
    "    if not np.all(eigenvalues >= 1e-6):\n",
    "        raise ValueError(f\"Matrix rho is not positive semi-definite. rho:\\n{np.linalg.eigh(rho)[0]} \\n {rho}\")\n",
    "    \n",
    "    # check if rho is Hermitian\n",
    "    if not np.allclose(rho, rho.conj().T):\n",
    "        raise ValueError(f\"Matrix rho is not Hermitian. rho: \\n{rho}\")\n",
    "    \n",
    "    # check if trace of rho is equal to 1\n",
    "    if not np.isclose(np.trace(rho), 1):\n",
    "        raise ValueError(f\"The trace of rho is not equal to 1. rho: {np.trace(rho)} \\n {rho}\")\n",
    "    \n",
    "    if check_negative:\n",
    "        # check if rho has negative elements\n",
    "        if not np.all(rho >= 0):\n",
    "            raise ValueError(f\"Matrix rho contains negative values: \\n{rho}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3eceb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_odd_y_interactions_to_zero(interaction_labels, interaction_weights):\n",
    "    # iterate over each label tuple\n",
    "    for label_array in interaction_labels:\n",
    "        # convert numpy array to tuple\n",
    "        label_tuple = tuple(label_array)\n",
    "        # count the number of 'Y' in the label tuple\n",
    "        num_y = label_tuple.count('Y')\n",
    "\n",
    "        # if the count is odd, set the corresponding weight to 0\n",
    "        if num_y % 2 == 1:\n",
    "            interaction_weights[label_tuple] = 0\n",
    "\n",
    "    return interaction_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f484c151",
   "metadata": {},
   "source": [
    "We now can write a function that checks if we're using valid QM parameters using the following rules:\n",
    "\n",
    "1. $\\sigma_{ij}^{xx} > \\sigma_{ij}^{yy} \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\forall  i,j$\n",
    "2. $\\sigma_{i}^{x} > \\sum_{j \\neq i} \\sigma_{ij}^{xy} \\ \\ \\ \\forall i,j$\n",
    "3. No odd Y terms\n",
    "4. $\\sigma^z_i$ < |1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74a465e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_qm_parameters():\n",
    "#     ###TO BE WRITTEN IF THIS IS SUFFICIENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a83f3af",
   "metadata": {},
   "source": [
    "## **Implementation of the Quantum Boltzmann Machine**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a28eba30",
   "metadata": {},
   "source": [
    "Now that we can properly generate the interaction matrices, associate a weight with them and obtain the density matrix. We can build the quantum boltzmann machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b40393e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###QBM AUXILLARY FUNCTIONS\n",
    "#----------------------------------------------------------------------------------------------\n",
    "@njit\n",
    "def observables(rho: np.ndarray, interaction_matrices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the expectation values of the interaction matrices [observables]\n",
    "    \"\"\"\n",
    "    obs = np.zeros(len(interaction_matrices), dtype=np.complex128)\n",
    "    rho_contig = np.ascontiguousarray(rho)\n",
    "\n",
    "    for i, interaction_matrix in enumerate(interaction_matrices):   \n",
    "        interaction_matrix_contig = np.ascontiguousarray(interaction_matrix)\n",
    "        obs[i] = np.real(np.trace(np.dot(rho_contig, interaction_matrix_contig)))\n",
    "    return obs\n",
    "\n",
    "@njit\n",
    "def KL_divergence(eta: np.ndarray, rho: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the KL divergence between the model and target distribution.\n",
    "    \"\"\"\n",
    "    return np.real(np.trace(eta@(logmat(eta)-logmat(rho))))\n",
    "\n",
    "@njit\n",
    "def QM_likelihood(eta: np.ndarray, rho: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the quantum Likelihood of the distribution\n",
    "    \"\"\"\n",
    "    return -np.real(np.trace(eta @ logmat(rho)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58b82f56",
   "metadata": {},
   "source": [
    "Now build the actual QBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3dfb49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def quantum_boltzmann_machine(\n",
    "    interaction_matrices: np.ndarray, \n",
    "    learning_rate: float, \n",
    "    maxiter: int, \n",
    "    tolerance: float, \n",
    "    w_initial: np.ndarray, \n",
    "    eta: np.ndarray = np.array([math.nan]), \n",
    "    w_eta: np.ndarray = np.array([math.nan])\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, int]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Train the Quantum Boltzmann Machine (QBM) to fit the target distribution eta.    \n",
    "    \"\"\"\n",
    "    # compute target density matrix using ED if oracle w_eta is provided, otherwise use provided eta\n",
    "    if np.isnan(eta).any():\n",
    "        assert not np.isnan(w_eta).any(), \"Either eta or w_eta must be provided\"\n",
    "        eta = rho_model(w_eta, interaction_matrices)\n",
    "\n",
    "    obs_clamped = observables(eta, interaction_matrices)  # get clamped QM statistics\n",
    "\n",
    "    w = w_initial.copy()  # get initial weights for free QM statistics\n",
    "    rho = rho_model(w, interaction_matrices)  # generate the density matrix rho for the initial weights\n",
    "    obs_model = observables(rho, interaction_matrices)  # get free QM statistics\n",
    "\n",
    "    it = 0  # initialize gradient ascent loop\n",
    "    diff = np.inf\n",
    "    W_list, lk_list, kl_list = np.zeros(maxiter), np.zeros(maxiter), np.zeros(maxiter)  # initialize values to store\n",
    "\n",
    "    consecutive_small_change = 0  # counter for consecutive iterations with small change in W_max\n",
    "    prev_W_diff = 0.0  # previous value of W_max\n",
    "\n",
    "    while diff > tolerance and it < maxiter and consecutive_small_change < 20:\n",
    "        rho = rho_model(w, interaction_matrices)  # get free QM statistics\n",
    "        obs_model = observables(rho, interaction_matrices)\n",
    "        w_previous = w.copy()\n",
    "        w += learning_rate * np.real(obs_clamped - obs_model)  # Update weights\n",
    "\n",
    "        if np.isnan(w_eta).any():\n",
    "            W_diff = np.max(np.abs(w - w_previous))\n",
    "            W_list[it] = W_diff\n",
    "\n",
    "        else:\n",
    "            Wmax = np.max(np.abs(w-w_eta))\n",
    "            W_list[it] = Wmax                  \n",
    "\n",
    "        # check if W_max has a small change compared to the previous iteration\n",
    "        if np.abs(W_diff - prev_W_diff) < 1e-10:\n",
    "            consecutive_small_change += 1\n",
    "        else:\n",
    "            consecutive_small_change = 0  # reset counter if W_max changes significantly\n",
    "        prev_W_diff = W_diff\n",
    "\n",
    "        diff = np.max(np.abs(obs_model - obs_clamped))  # evaluate differences in clamped and model statistics\n",
    "\n",
    "        lk_list[it] = QM_likelihood(eta, rho)\n",
    "        kl_list[it] = KL_divergence(eta, rho)\n",
    "        it += 1\n",
    "    \n",
    "    return w, lk_list, kl_list, W_list, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59e1d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  PLOTTING FUNCTIONS\n",
    "#----------------------------------------------------------------------\n",
    "def plot(it, Wmax, kl, lk, title='Convergence Plots', size=10):\n",
    "    fig = plt.figure(figsize=(25, 6));                           #  make plots\n",
    "    fig.suptitle(title, fontsize=30, y = 1)\n",
    "    trans = mtransforms.ScaledTranslation(-20/72, 7/72, fig.dpi_scale_trans)\n",
    "    its= np.arange(1,it+1,1)\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax1.scatter(its, Wmax[:it], s=size, marker='o', color=\"SlateBlue\")\n",
    "    ax1.set_xlabel(\"Iterations\", fontsize=20)\n",
    "    ax1.set_ylabel(r\"$ (\\Delta w)_{max}$\", fontsize=20)\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.text(0, 1.0, 'A.)', transform=ax1.transAxes + trans, fontsize='large',fontweight ='bold', va='bottom', fontfamily='sans-serif')\n",
    "   \n",
    "\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax2.scatter(its, kl[:it], s=size, marker='o', color=\"ForestGreen\")\n",
    "    ax2.set_xlabel(\"Iterations\", fontsize=20)\n",
    "    ax2.set_ylabel(\"KL Divergence\", fontsize=20)\n",
    "    ax2.set_yscale('symlog')  #symlog for when the KL divergence goes to zero\n",
    "    ax2.text(0, 1.0, 'B.)', transform=ax2.transAxes + trans, fontsize='large',fontweight ='bold', va='bottom', fontfamily='sans-serif')\n",
    "\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    ax3.scatter(its, lk[:it], s=size, marker='o', color=\"Coral\")\n",
    "    ax3.set_xlabel(\"Iterations\", fontsize=20)\n",
    "    ax3.set_ylabel(\"Log Likelihood\", fontsize=20)\n",
    "    ax3.set_yscale('log')\n",
    "    # ax3.set_ylim([0.482015, 0.482030])\n",
    "    ax3.text(0, 1.0, 'B.)', transform=ax3.transAxes + trans, fontsize='large',fontweight ='bold', va='bottom', fontfamily='sans-serif')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "514281ad",
   "metadata": {},
   "source": [
    "## **Bidirectional Conversion of Density Matrices and Transition Matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c45b0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def transition_matrix_to_density_matrix(W: np.ndarray) -> np.ndarray:\n",
    "    # calculate steady-state distribution p\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(W.T) #switch row to collumn normalized by transposing\n",
    "\n",
    "    p = np.abs(eigenvectors[:, np.argmax(np.real(eigenvalues))])\n",
    "    p = p / np.sum(p) #normalize \n",
    "\n",
    "    # calculate the matrix A\n",
    "    sqrt_p_diag     = np.diag(np.sqrt(p))\n",
    "    inv_sqrt_p_diag = np.diag(1 / np.sqrt(p))\n",
    "    \n",
    "    A = inv_sqrt_p_diag @ W.T @ sqrt_p_diag\n",
    "\n",
    "    # symmetrize A to make it Hermitian\n",
    "    A = 0.5 * (A + np.transpose(np.conjugate(A)))\n",
    "\n",
    "    # add a scalar constant to all eigenvalues to make them non-negative (ensuring rho is positive semidefinite)\n",
    "    min_eigval = np.min(np.real(np.linalg.eigvals(A)))\n",
    "    if min_eigval < 0:\n",
    "        A += (np.abs(min_eigval) + 1e-4) * np.eye(A.shape[0])\n",
    "\n",
    "    #  normalize such that Tr[rho] = 1\n",
    "    rho = A / np.real(np.trace(A))    \n",
    "    rho = rho.astype(np.complex128) #make it complex    \n",
    "\n",
    "    return rho\n",
    "\n",
    "@njit\n",
    "def density_matrix_to_transition_matrix(rho: np.ndarray) -> np.ndarray:\n",
    "    # obtain the steady-state distribution √p\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(rho)\n",
    "    sqrt_p = np.abs(eigenvectors[:, np.argmax(np.real(eigenvalues))])\n",
    "\n",
    "    # square the elements of the eigenvector and normalize to obtain the steady-state distribution\n",
    "    p = sqrt_p**2\n",
    "    p = p / np.sum(p)  # normalize\n",
    "    epsilon = 1e-10  # small constant to prevent division by zero\n",
    "    p = p + epsilon\n",
    "\n",
    "    sqrt_p_diag     = np.diag(np.sqrt(p))\n",
    "    inv_sqrt_p_diag = np.diag(1 / np.sqrt(p))\n",
    "\n",
    "    # calculate the matrix A\n",
    "    A = np.real(rho)\n",
    "\n",
    "    # compute the transition matrix W \n",
    "    W = sqrt_p_diag @ A @ inv_sqrt_p_diag\n",
    "    W = W.T #switch back to row normalized by transposing\n",
    "\n",
    "    # normalize each row to ensure the sum of each row equals 1\n",
    "    W_row_sum = W.sum(axis=1)\n",
    "    for i in range(W.shape[0]):\n",
    "        W[i, :] /= W_row_sum[i]\n",
    "\n",
    "    return W\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e3c3306",
   "metadata": {},
   "source": [
    "## **Quantum-Classical Mapping**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41ea891f",
   "metadata": {},
   "source": [
    "### Building the pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "543cd048",
   "metadata": {},
   "source": [
    "We can build a pipeline for which we get input classical parameters and output qm parameters. We can also do the opposite, map qm parameters to classical ones. We call this the inverse route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92601ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_mapping(w: np.ndarray, J: np.ndarray, h: np.ndarray, learning_rate_qbm: float, maxiter_qbm: int, plot_convergence = False, perform_checks = True) -> np.ndarray:\n",
    "    N = len(h)\n",
    "\n",
    "    #get the transition matrix\n",
    "    W = compute_transition_matrix(w,h,J)\n",
    "\n",
    "    #convert it to a target density matrix\n",
    "    eta = transition_matrix_to_density_matrix(W)\n",
    "\n",
    "    # test if W and eta are valid\n",
    "    if perform_checks:\n",
    "        test_transition_matrix(W)\n",
    "        check_density_matrix(eta)\n",
    "        \n",
    "    # learning parameters qbm\n",
    "    tolerance = 1e-14\n",
    "    random_seed = 444\n",
    "\n",
    "    # set initial weights for QBM\n",
    "    interaction_matrices, interaction_labels, _ = generate_interaction_matrices(N)\n",
    "    w_initial= generate_random_parameter_matrix(random_seed + 1, N, interaction_labels)\n",
    "\n",
    "    #infer its parameters using the QBM\n",
    "    w_qm, lk, kl, Wdiff, it = quantum_boltzmann_machine(interaction_matrices, learning_rate_qbm, maxiter_qbm, tolerance, w_initial, eta=eta)\n",
    "    \n",
    "    #plot its convergence if needed\n",
    "    if plot_convergence:\n",
    "        plot(it, Wdiff, kl, lk, title='QBM Convergence', size=10)\n",
    "\n",
    "    return w_qm\n",
    "\n",
    "\n",
    "def inverse_mapping(w_qm: np.ndarray, learning_rate_bm: float, maxiter_bm: int, plot_convergence = False, perform_checks = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    N = int(np.log(w_qm.shape[0]) / np.log(4))\n",
    "\n",
    "    #qm hamiltonian to density matrix\n",
    "    interaction_matrices, _, _ = generate_interaction_matrices(N)\n",
    "    rho = rho_model(w_qm, interaction_matrices)\n",
    "\n",
    "    #density matrix to transition matrix\n",
    "    W = density_matrix_to_transition_matrix(rho)\n",
    "\n",
    "    # test if rho and W are valid\n",
    "    if perform_checks:\n",
    "        check_density_matrix(rho)\n",
    "        test_transition_matrix(W)\n",
    "\n",
    "    #infer the classical parameters using a classical boltzmann machine\n",
    "    w, J, h = infer_parameters(W, maxiter_bm, learning_rate_bm, plot = plot_convergence)\n",
    "    \n",
    "    return w, J, h\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7555b516",
   "metadata": {},
   "source": [
    "### Plotting the mappings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1bbff2b",
   "metadata": {},
   "source": [
    "We'd like to be able to make plots to see how quantum parameters change when classical parameters are varied and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99207dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qm_interaction_label_to_string(interaction_label: Tuple[str, ...]) -> str:\n",
    "    \"\"\"Convert an quantum interaction label tuple to a string (h, J or sigma)\"\"\"\n",
    "    # Find indices of all non-I terms\n",
    "    non_i_indices = [i for i, val in enumerate(interaction_label) if val != 'I']\n",
    "    # Generate string based on number of non-I terms\n",
    "    if len(non_i_indices) == 0:\n",
    "        return f\"$\\log(Z)$\"\n",
    "    elif len(non_i_indices) == 1:\n",
    "        return f\"$h_{{{non_i_indices[0]+1}}}^{{{interaction_label[non_i_indices[0]]}}}$\"\n",
    "    elif len(non_i_indices) == 2:\n",
    "        return f\"$J_{{{non_i_indices[0]+1}{non_i_indices[1]+1}}}^{{{interaction_label[non_i_indices[0]]}{interaction_label[non_i_indices[1]]}}}$\"\n",
    "    else:\n",
    "        i_terms = \"\".join([str(idx+1) for idx in non_i_indices])\n",
    "        k_terms = \"\".join([interaction_label[idx] for idx in non_i_indices])\n",
    "        return f\"$\\\\sigma_{{{i_terms}}}^{{{k_terms}}}$\"\n",
    "\n",
    "def qm_string_to_interaction_label(label_string: str, N: int) -> Tuple[str, ...]:\n",
    "    \"\"\"Convert a quantum string label to an interaction label tuple.\"\"\"\n",
    "    # check if the string starts with 'l' (log Z)\n",
    "    if label_string.startswith('l'):\n",
    "        return tuple('I' for _ in range(N))\n",
    "\n",
    "    # initialize a list of 'I's of length N\n",
    "    interaction_label = ['I'] * N\n",
    "\n",
    "    # check if the string starts with 'h' (bias)\n",
    "    if label_string.startswith('h'):\n",
    "        # use regex to find the indices and axis in the label string\n",
    "        m = re.match(r\"h_\\{(\\d+)\\}\\^\\{(\\w)\\}\", label_string)\n",
    "        index = int(m.group(1))\n",
    "        axis = m.group(2)\n",
    "\n",
    "        # replace the corresponding 'I' with axis\n",
    "        interaction_label[index - 1] = axis\n",
    "\n",
    "    # check if the string starts with 'J' (two-spin interaction)\n",
    "    elif label_string.startswith('J'):\n",
    "        m = re.match(r\"J_\\{(\\d+)(\\d+)\\}\\^\\{(\\w)(\\w)\\}\", label_string)\n",
    "        indices = (int(m.group(1)), int(m.group(2)))\n",
    "        axes = (m.group(3), m.group(4))\n",
    "\n",
    "        # replace the corresponding 'I's with axes\n",
    "        for i in range(2):\n",
    "            interaction_label[indices[i] - 1] = axes[i]\n",
    "\n",
    "    # otherwise, assume the string starts with '\\sigma' (multi-spin interaction)\n",
    "    else:\n",
    "        m = re.match(r\"\\\\sigma_\\{(.+)\\}\\^\\{(.+)\\}\", label_string)\n",
    "        indices = tuple(map(int, m.group(1)))\n",
    "        axes = tuple(m.group(2))\n",
    "\n",
    "        # replace the corresponding 'I's with axes\n",
    "        for i in range(len(indices)):\n",
    "            interaction_label[indices[i] - 1] = axes[i]\n",
    "\n",
    "    return tuple(interaction_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a52cd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_forward_parameter_mapping(start: float, end: float, steps: int, param: str, index: Tuple[int], w: np.ndarray, J: np.ndarray, h: np.ndarray, learning_rate_qbm: float, maxiter_qbm: int, learning_rate_bm: float, maxiter_bm: int):\n",
    "    \"\"\"Plot change of QM parameters when a single classical parameter is varied.\"\"\"\n",
    "    # check that the input matrices are symmetric and J has a zero diagonal\n",
    "    check_parameters(w, J, h)\n",
    "\n",
    "    # create array of parameter values to test\n",
    "    qm_param_values = np.linspace(start, end, steps)\n",
    "\n",
    "    # create lists to hold quantum parameter values\n",
    "    quantum_params = []\n",
    "\n",
    "    N = len(h)\n",
    "    _, interaction_labels, _ = generate_interaction_matrices(N)\n",
    "\n",
    "    for idx, value in enumerate(qm_param_values):\n",
    "        # update the corresponding parameter\n",
    "        if param == 'w':\n",
    "            w[index] = value\n",
    "            w[index[::-1]] = value    # update symmetric element\n",
    "        elif param == 'J':\n",
    "            J[index] = value\n",
    "            J[index[::-1]] = value    # update symmetric element\n",
    "        elif param == 'h':\n",
    "            h[index[0]] = value\n",
    "\n",
    "        # check if new parameters are valid\n",
    "        check_parameters(w, J, h)\n",
    "        \n",
    "        # forward mapping [only plot the last iteration]\n",
    "        w_qm = forward_mapping(w, J, h, learning_rate_qbm, maxiter_qbm, plot_convergence = False)\n",
    "\n",
    "        # add to list\n",
    "        quantum_params.append(w_qm)\n",
    "\n",
    "    # check for inversibility of last iteration\n",
    "    w_recovered, J_recovered, h_recovered = inverse_mapping(w_qm, learning_rate_bm, maxiter_bm, plot_convergence= False, perform_checks = False)\n",
    "    assert (np.allclose(w, w_recovered, atol = .1) and np.allclose(J, J_recovered, atol = .1) and np.allclose(h, h_recovered, atol = .1)), f\"Mapping is not inversible.\"\n",
    "\n",
    "\n",
    "    return quantum_params\n",
    "\n",
    "def plot_forward_mapping(quantum_params, start, end, steps, param, index, N, qm_params_to_plot=None, threshold=0.01):\n",
    "    qm_param_values = np.linspace(start, end, steps)\n",
    "    _, interaction_labels, _ = generate_interaction_matrices(N)\n",
    "\n",
    "    # Dictionary to map string labels to color\n",
    "    label_color_map = {}\n",
    "\n",
    "    _, interaction_labels, _ = generate_interaction_matrices(N)\n",
    "\n",
    "    qm_str_labels = [qm_interaction_label_to_string(label) for label in interaction_labels]\n",
    "\n",
    "    h_count = 0\n",
    "    J_count = 0\n",
    "    sigma_count = 0\n",
    "\n",
    "    for qm_str_label in qm_str_labels:\n",
    "        # Check if the string starts with 'h' (bias)\n",
    "        if qm_str_label.startswith('h'):\n",
    "            h_count += 1\n",
    "            label_color_map[qm_str_label] = plt.cm.Dark1(h_count)\n",
    "\n",
    "        # Check if the string starts with 'J' (two-spin interaction)\n",
    "        elif qm_str_label.startswith('J'):\n",
    "            J_count += 1\n",
    "            label_color_map[qm_str_label] = plt.cm.Accent(J_count)\n",
    "\n",
    "        # Otherwise, assume the string starts with '\\sigma' (multi-spin interaction)\n",
    "        else:\n",
    "            sigma_count += 1\n",
    "            label_color_map[qm_str_label] = plt.cm.Set3(sigma_count)\n",
    "\n",
    "    # create plots for the changed parameters\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # find quantum parameters that have changed more than the threshold\n",
    "    changed_params_booleans = np.abs(np.max(quantum_params, axis=0) - np.min(quantum_params, axis=0)) > threshold\n",
    "\n",
    "    # get tuple labels to plot\n",
    "    tuples_to_plot = np.array(interaction_labels)[changed_params_booleans]\n",
    "\n",
    "    #convert tuple labels to strings\n",
    "    qm_str_labels_to_plot = [qm_interaction_label_to_string(label) for label in tuples_to_plot]\n",
    "\n",
    "    #sort them alphabetically and numerically\n",
    "    sorted_qm_str_labels_to_plot = sorted(qm_str_labels_to_plot, key=lambda x: (x[0].isalpha(), x))\n",
    "\n",
    "    print(sorted_qm_str_labels_to_plot)\n",
    "    #convert them back to tuple labels\n",
    "    sorted_tuples_to_plot = [qm_string_to_interaction_label(string, N) for string in sorted_qm_str_labels_to_plot]\n",
    "\n",
    "    for i, label in enumerate(sorted_tuples_to_plot):\n",
    "        qm_str_label = qm_interaction_label_to_string(label)\n",
    "        classical_param_values = (np.array(quantum_params)[:, changed_params_booleans])[:, i]\n",
    "\n",
    "        if qm_params_to_plot is None or qm_str_label in qm_params_to_plot:\n",
    "            color = label_color_map[qm_str_label]\n",
    "            plt.plot(qm_param_values, classical_param_values, label=qm_str_label, color=color)\n",
    "\n",
    "    classical_param_index_str = \"\".join(str(idx + 1) for idx in index)  # convert to proper string for visualization\n",
    "    plt.title(f'Variation of Quantum Parameters with Classical Parameter ${param}_{{ {classical_param_index_str} }}$')\n",
    "    plt.xlabel(f'${param}_{{ {classical_param_index_str} }}$')\n",
    "    plt.ylabel('Quantum Parameters')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def analyze_inverse_parameter_mapping(start: float, end: float, steps: int, quantum_param_label: str, w_qm: np.ndarray, learning_rate_bm: float, maxiter_bm: int, classical_params_to_plot = None, threshold=0.01):\n",
    "    \"\"\"Plot change of Classical parameters when a single Quantum parameter is varied.\"\"\"\n",
    "    N  = int(np.log(w_qm.shape[0]) / np.log(4))\n",
    "    \n",
    "    # create array of quantum parameter values to test\n",
    "    quantum_param_values = np.linspace(start, end, steps)\n",
    "\n",
    "    # create lists to hold classical parameter values\n",
    "    w_vals = []\n",
    "    J_vals = []\n",
    "    h_vals = []\n",
    "\n",
    "    # find the index of quantum_param_label in interaction_labels\n",
    "    _, _, interaction_weights = generate_interaction_matrices(N)\n",
    "    param_index = list(interaction_weights.keys()).index((quantum_param_label))     \n",
    "\n",
    "    for idx, value in enumerate(quantum_param_values):\n",
    "        # update the quantum parameter\n",
    "        w_qm[param_index] = value\n",
    "\n",
    "        # perform the inverse mapping [only plot the last iteration]\n",
    "        w, J, h = inverse_mapping(w_qm, learning_rate_bm, maxiter_bm, plot_convergence = (idx == len(quantum_param_values)-1))\n",
    "\n",
    "        # add to lists\n",
    "        w_vals.append(w)\n",
    "        J_vals.append(J)\n",
    "        h_vals.append(h)\n",
    "\n",
    "    # find classical parameters that have changed sufficiently\n",
    "    changed_w = np.abs(np.max(np.diff(w_vals, axis=0), axis=0)) > threshold\n",
    "    changed_J = np.abs(np.max(np.diff(J_vals, axis=0), axis=0)) > threshold\n",
    "    changed_h = np.abs(np.max(np.diff(h_vals, axis=0), axis=0)) > threshold\n",
    "\n",
    "    # flatten parameters\n",
    "    w_vals = np.array(w_vals).reshape(steps,N**2)\n",
    "    J_vals = np.array(J_vals).reshape(steps,N**2)\n",
    "    h_vals = np.array(h_vals)\n",
    "    changed_w = changed_w.flatten()\n",
    "    changed_J = changed_J.flatten()\n",
    "    changed_h = changed_h.flatten()\n",
    "\n",
    "    # if specific parameters to plot are provided, use those\n",
    "    if classical_params_to_plot is not None:\n",
    "            print('this is not implemented yet!')\n",
    "\n",
    "    # create plots for the changed parameters\n",
    "    fig = plt.figure(figsize=(25, 6));     #  make plots\n",
    "    qm_str_label = qm_interaction_label_to_string(quantum_param_label) #get string label of sweep qm parameter\n",
    "\n",
    "    fig.suptitle(f'Variation of Classical Paramaters versus Quantum Parameter {qm_str_label}', fontsize=30, y = 1.1)\n",
    "    trans = mtransforms.ScaledTranslation(-20/72, 7/72, fig.dpi_scale_trans)\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    for i, w_vals_changed in enumerate(w_vals[:,changed_w].T):\n",
    "        original_index = np.unravel_index(i, (N, N))\n",
    "        classical_param_index_str =\"\".join(str(idx + 1) for idx in original_index) #convert to proper string for visualization\n",
    "        ax1.scatter(quantum_param_values, w_vals_changed, label= f'$w_{{ {classical_param_index_str} }}$', s = 15)\n",
    "    ax1.set_xlabel(f'{qm_str_label}', fontsize=20)\n",
    "    ax1.set_ylabel('$w$ parameters', fontsize=20)\n",
    "    ax1.legend()\n",
    "    ax1.text(0, 1.0, 'A.)', transform=ax1.transAxes + trans, fontsize='large',fontweight ='bold', va='bottom', fontfamily='sans-serif')\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    for i, J_vals_changed in enumerate(J_vals[:,changed_J].T):\n",
    "        original_index = np.unravel_index(i, (N, N))\n",
    "        classical_param_index_str =\"\".join(str(idx + 1) for idx in original_index) #convert to proper string for visualization\n",
    "        ax2.scatter(quantum_param_values, J_vals_changed, label= f'$J_{{ {classical_param_index_str} }}$', s = 15)\n",
    "    ax2.set_xlabel(f'{qm_str_label}', fontsize=20)\n",
    "    ax2.set_ylabel('$J$ parameters', fontsize=20)\n",
    "    ax2.legend()\n",
    "    ax2.text(0, 1.0, 'B.)', transform=ax2.transAxes + trans, fontsize='large',fontweight ='bold', va='bottom', fontfamily='sans-serif')\n",
    "\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    plt.figure(figsize=(4,3))\n",
    "    for i, h_vals_changed in enumerate(h_vals[:,changed_h].T):\n",
    "        original_index = np.unravel_index(i, (N, N))\n",
    "        classical_param_index_str =\"\".join(str(idx + 1) for idx in original_index) #convert to proper string for visualization\n",
    "        ax3.scatter(quantum_param_values, h_vals_changed, label= f'$h_{{ {classical_param_index_str} }}$', s = 15)\n",
    "    ax3.set_xlabel(f'{qm_str_label}', fontsize=20)\n",
    "    ax3.set_ylabel('$h$ parameters', fontsize=20)\n",
    "    ax3.legend()\n",
    "    ax3.text(0, 1.0, 'B.)', transform=ax3.transAxes + trans, fontsize='large',fontweight ='bold', va='bottom', fontfamily='sans-serif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_parameter_sweep(start, end, steps, param, index, w, J, h,\n",
    "                            learning_rate_qbm, maxiter_qbm,\n",
    "                            learning_rate_bm, maxiter_bm):\n",
    "    \"\"\"\n",
    "    Perform a parameter sweep and plot the absolute inversion error.\n",
    "    \"\"\"\n",
    "\n",
    "    qm_param_values = np.linspace(start, end, steps)\n",
    "    inversion_error = []\n",
    "\n",
    "    for idx, value in enumerate(qm_param_values):\n",
    "        if param == 'w':\n",
    "            w[index] = value\n",
    "            w[index[::-1]] = value\n",
    "        elif param == 'J':\n",
    "            J[index] = value\n",
    "            J[index[::-1]] = value\n",
    "        elif param == 'h':\n",
    "            h[index[0]] = value\n",
    "\n",
    "        check_parameters(w, J, h)\n",
    "\n",
    "        w_qm = forward_mapping(w, J, h, learning_rate_qbm, maxiter_qbm, plot_convergence=False)\n",
    "        w_recovered, J_recovered, h_recovered = inverse_mapping(w_qm, learning_rate_bm, maxiter_bm, plot_convergence=False)\n",
    "        \n",
    "        error = np.sum(np.abs(w - w_recovered)) + np.sum(np.abs(J - J_recovered)) + np.sum(np.abs(h - h_recovered))\n",
    "        inversion_error.append(error)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(qm_param_values, inversion_error)\n",
    "    classical_param_index_str = \"\".join(str(idx + 1) for idx in index)\n",
    "    plt.title(f'Absolute Inversion error when varying ${param}_{{ {classical_param_index_str} }}$')\n",
    "    plt.xlabel(f'${param}_{{ {classical_param_index_str} }}$')\n",
    "    plt.ylabel('Sum of absolute error between initial and recovered parameters')\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e3fd9d2",
   "metadata": {},
   "source": [
    "As a test, we check when our mapping does not become inversible. We should not use those ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_parameter_sweep(start, end, steps, param, index, w, J, h,\n",
    "                            learning_rate_qbm, maxiter_qbm,\n",
    "                            learning_rate_bm, maxiter_bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28522bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classical parameters\n",
    "w = np.array([[14, 14], \n",
    "              [14, 14]], dtype=np.float64)\n",
    "J = np.array([[0, 1],\n",
    "              [1, 0]],   dtype=np.float64)\n",
    "h = np.array([1, -1],    dtype=np.float64)\n",
    "\n",
    "# make sure w are large enough for normalized probabilities\n",
    "check_parameters(w,J,h)\n",
    "\n",
    "#intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_qbm = .5\n",
    "maxiter_qbm = 2**10\n",
    "\n",
    "learning_rate_bm = .9\n",
    "maxiter_bm = 2**20\n",
    "\n",
    "#initialize sweep \n",
    "param = 'J'\n",
    "index = (0,1)\n",
    "steps = 30\n",
    "\n",
    "# create array of parameter values to test. \n",
    "#THIS IS THE MAXIMUM SWEEPING RANGE ALLOWED\n",
    "qm_param_values = np.linspace(-11.95, 11.95, steps)\n",
    "\n",
    "# create lists to hold the error sizes\n",
    "inversion_error = []\n",
    "\n",
    "N = len(h)\n",
    "_, interaction_labels, _ = generate_interaction_matrices(N)\n",
    "\n",
    "for idx, value in enumerate(qm_param_values):\n",
    "    # update the corresponding parameter\n",
    "    if param == 'w':\n",
    "        w[index] = value\n",
    "        w[index[::-1]] = value    # update symmetric element\n",
    "    elif param == 'J':\n",
    "        J[index] = value\n",
    "        J[index[::-1]] = value    # update symmetric element\n",
    "    elif param == 'h':\n",
    "        h[index[0]] = value\n",
    "\n",
    "    # check if new parameters are valid\n",
    "    check_parameters(w, J, h)\n",
    "    \n",
    "    # forward mapping \n",
    "    w_qm = forward_mapping(w, J, h, learning_rate_qbm, maxiter_qbm, plot_convergence = False)\n",
    "\n",
    "    # check for inversibility of last iteration\n",
    "    w_recovered, J_recovered, h_recovered = inverse_mapping(w_qm, learning_rate_bm, maxiter_bm, plot_convergence= False)\n",
    "    \n",
    "    #compute the absolute error\n",
    "    error = np.sum(np.abs(w - w_recovered)) + np.sum(np.abs(J - J_recovered)) + np.sum(np.abs(h - h_recovered))\n",
    "\n",
    "    # add to list\n",
    "    inversion_error.append(error)\n",
    "\n",
    "# create plots for the error\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.plot(qm_param_values, inversion_error)\n",
    "\n",
    "classical_param_index_str =\"\".join(str(idx + 1) for idx in index) #convert to proper string for visualization\n",
    "plt.title(f'Absolute Inversion error when varying ${param}_{{ {classical_param_index_str} }}$')\n",
    "plt.xlabel(f'${param}_{{ {classical_param_index_str} }}$')\n",
    "plt.ylabel('Sum of absolute error between initial and recovered parameters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classical parameters\n",
    "w = np.array([[14, 14], \n",
    "              [14, 14]], dtype=np.float64)\n",
    "J = np.array([[0, 1],\n",
    "              [1, 0]],  dtype=np.float64)\n",
    "h = np.array([1, -1],    dtype=np.float64)\n",
    "\n",
    "# make sure w are large enough for normalized probabilities\n",
    "check_parameters(w,J,h)\n",
    "\n",
    "#intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_qbm = .5\n",
    "maxiter_qbm = 2**10\n",
    "\n",
    "learning_rate_bm = .9\n",
    "maxiter_bm = 2**20\n",
    "\n",
    "#initialize sweep \n",
    "param = 'J'\n",
    "index = (0,1)\n",
    "start = -11.95\n",
    "end   = 11.95\n",
    "steps = 30\n",
    "\n",
    "\n",
    "perform_parameter_sweep(start, end, steps, param, index, w, J, h,\n",
    "                            learning_rate_qbm, maxiter_qbm,\n",
    "                            learning_rate_bm, maxiter_bm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14de1856",
   "metadata": {},
   "source": [
    "Then we try for relatively small values of w and J but still going to the max allowed values of J. Then we can see if the error has to do with numerical precision or with going to the edge of normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classical parameters\n",
    "w = np.array([[3,3], \n",
    "              [3,3]], dtype=np.float64)\n",
    "J = np.array([[0, 2.21],\n",
    "              [2.21, 0]],  dtype=np.float64)\n",
    "h = np.array([.1, -.1],    dtype=np.float64)\n",
    "\n",
    "# make sure w are large enough for normalized probabilities\n",
    "check_parameters(w,J,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classical parameters\n",
    "w = np.array([[3,3], \n",
    "              [3,3]], dtype=np.float64)\n",
    "J = np.array([[0, .1],\n",
    "              [.1, 0]],  dtype=np.float64)\n",
    "h = np.array([.1, -.1],    dtype=np.float64)\n",
    "\n",
    "# make sure w are large enough for normalized probabilities\n",
    "check_parameters(w,J,h)\n",
    "\n",
    "#intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_qbm = .5\n",
    "maxiter_qbm = 2**10\n",
    "\n",
    "learning_rate_bm = .9\n",
    "maxiter_bm = 2**20\n",
    "\n",
    "#initialize sweep \n",
    "param = 'J'\n",
    "index = (0,1)\n",
    "steps = 50\n",
    "\n",
    "# create array of parameter values to test\n",
    "qm_param_values = np.linspace(-2.19, 2.19, steps)\n",
    "\n",
    "# create lists to hold the error sizes\n",
    "inversion_error = []\n",
    "\n",
    "N = len(h)\n",
    "_, interaction_labels, _ = generate_interaction_matrices(N)\n",
    "\n",
    "for idx, value in enumerate(qm_param_values):\n",
    "    # update the corresponding parameter\n",
    "    if param == 'w':\n",
    "        w[index] = value\n",
    "        w[index[::-1]] = value    # update symmetric element\n",
    "    elif param == 'J':\n",
    "        J[index] = value\n",
    "        J[index[::-1]] = value    # update symmetric element\n",
    "    elif param == 'h':\n",
    "        h[index[0]] = value\n",
    "\n",
    "    # check if new parameters are valid\n",
    "    check_parameters(w, J, h)\n",
    "    \n",
    "    # forward mapping \n",
    "    w_qm = forward_mapping(w, J, h, learning_rate_qbm, maxiter_qbm, plot_convergence = False)\n",
    "\n",
    "    # check for inversibility of last iteration\n",
    "    w_recovered, J_recovered, h_recovered = inverse_mapping(w_qm, learning_rate_bm, maxiter_bm, plot_convergence= False)\n",
    "    \n",
    "    #compute the absolute error\n",
    "    error = np.sum(np.abs(w - w_recovered)) + np.sum(np.abs(J - J_recovered)) + np.sum(np.abs(h - h_recovered))\n",
    "\n",
    "    # add to list\n",
    "    inversion_error.append(error)\n",
    "\n",
    "# create plots for the error\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.plot(qm_param_values, inversion_error)\n",
    "\n",
    "classical_param_index_str =\"\".join(str(idx + 1) for idx in index) #convert to proper string for visualization\n",
    "plt.title(f'Absolute Inversion error when varying ${param}_{{ {classical_param_index_str} }}$')\n",
    "plt.xlabel(f'${param}_{{ {classical_param_index_str} }}$')\n",
    "plt.ylabel('Sum of absolute error between initial and recovered parameters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "783e3df4",
   "metadata": {},
   "source": [
    "# **Results Quantum-Classical Mapping**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03aed1cb",
   "metadata": {},
   "source": [
    "**We now got all tools available to analyze some results of the parameter mapping.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66b3ff96",
   "metadata": {},
   "source": [
    "THINGS TO CHECK:\n",
    "- Try homogenous systems first [all parameters of a kind the same]\n",
    "- Check biases <-> sigma z correspondence\n",
    "- Make a way to systematically review the change in parameters. Plots of the change in parameters versus the change in an other.\n",
    "- Interpret wyy, wxx terms.\n",
    "- Parameters of a dynamics system map to a stationary thing?\n",
    "- Check if KL goes to zero for larger systems and quantum hamiltonian with 2body interactions.\n",
    "- Get same graphs as PPT dynamics\n",
    "- Investigate non-zero 3 body interactions in QM Hamiltonian\n",
    "- What quantum information do we lose when transfering it to a classical object?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2538cfb7",
   "metadata": {},
   "source": [
    "## Forward Reversibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "856da6f8",
   "metadata": {},
   "source": [
    "### Reversibility for 2-qubits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d750f10",
   "metadata": {},
   "source": [
    "Since for a 2-qubit system, the Hamiltonian is a complete model, we would expect that the mapping $w_{cl} \\rarr w_{qm} \\rarr w_{cl}$ works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c291da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classical parameters\n",
    "N = 2\n",
    "w = np.array([[8,9],\n",
    "              [9,8]], dtype=np.float64)\n",
    "J = np.array([[0,4],\n",
    "              [4,0]], dtype=np.float64)\n",
    "h = np.array([0, 2],  dtype=np.float64)\n",
    "\n",
    "# make sure w are large enough for normalized probabilities\n",
    "check_parameters(w,J,h)\n",
    "\n",
    "#intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_qbm = .2\n",
    "maxiter_qbm = 2**8\n",
    "\n",
    "learning_rate_bm = .9\n",
    "maxiter_bm = 2**20\n",
    "\n",
    "# map forward and back\n",
    "w_qm = forward_mapping(w, J, h, learning_rate_qbm, maxiter_qbm, plot_convergence = True)\n",
    "w_recovered, J_recovered, h_recovered = inverse_mapping(w_qm, learning_rate_bm, maxiter_bm, plot_convergence = True)\n",
    "\n",
    "print('Difference in initial and recovered parameters:')\n",
    "print(f'w: \\n {np.abs(w - w_recovered)}')\n",
    "print(f'J: \\n {np.abs(J - J_recovered)}')\n",
    "print(f'h: \\n {np.abs(h - h_recovered)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1197d3b",
   "metadata": {},
   "source": [
    "And so it does. Interestingly, for the same learning rate, the QBM converges WAY faster,but can only converge up to 1e-6. BM can converge all the way till 1e-14.  The QBM takes about 50 iterations to converge to 1e-6 while the BM takes ...."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c101d9c",
   "metadata": {},
   "source": [
    "### Reversibility for 3-qubits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2711261b",
   "metadata": {},
   "source": [
    "\n",
    "Another open question is if we are be able to invert the forward mapping for a non-complete Hamiltonian (i.e. N > 2). Let's try for a 3-qubit system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09735953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classical parameters\n",
    "N = 3\n",
    "w = np.array([[12   , 10.01, 12.01],\n",
    "              [10.01, 12.01,  9.01],\n",
    "              [12.01,  9.01, 12.01]] , dtype=np.float64)\n",
    "J = np.array([[0,1,2],\n",
    "              [1,0,1],\n",
    "              [2,1,0]], dtype=np.float64)\n",
    "h = np.array([1, 2, 3], dtype=np.float64)\n",
    "\n",
    "check_parameters(w,J,h)\n",
    "\n",
    "\n",
    "#intialize the optimal learning parameters for 3-qubits\n",
    "learning_rate_qbm = .9\n",
    "maxiter_qbm = 2**10\n",
    "\n",
    "learning_rate_bm = .9\n",
    "maxiter_bm = 2**20\n",
    "\n",
    "# map forward and back\n",
    "w_qm = forward_mapping(w, J, h, learning_rate_qbm, maxiter_qbm, plot_convergence = True, perform_checks = True)\n",
    "w_recovered, J_recovered, h_recovered = inverse_mapping(w_qm, learning_rate_bm, maxiter_bm, plot_convergence = True, perform_checks = False)\n",
    "\n",
    "print('Difference in initial and recovered parameters:')\n",
    "print(f'w: \\n {np.abs(w - w_recovered)}')\n",
    "print(f'J: \\n {np.abs(J - J_recovered)}')\n",
    "print(f'h: \\n {np.abs(h - h_recovered)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7920a520",
   "metadata": {},
   "source": [
    "- We can recover the parameters, pretty accurately, even though we don't use a complete Hamiltonian. Note that we do have to turn of the checking of valid W and rho because some values get slighly negative due inaccuracies.\n",
    "- Don't quite understand what happens with the KL-divergence though. This effect seems to occur no matter how low a learning rate is chosen (e.g. for  $\\alpha_{QBM} = 0.001$ this still happens)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5834da12",
   "metadata": {},
   "source": [
    "### Reversibility for 4-qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa133548",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_run = 0  #skip this due to long computation time\n",
    "if dont_run == 1: \n",
    "\n",
    "    # initialize the classical parameters\n",
    "    N = 4\n",
    "    w = np.array([[5.20,5.24,5.26,5.17],\n",
    "                  [5.24,5.26,5.20,5.19],\n",
    "                  [5.26,5.20,5.22,5.20],\n",
    "                  [5.17,5.19,5.20,5.20]], dtype=np.float64)\n",
    "    \n",
    "    J = np.array([[0,.1,.2,.3],\n",
    "                  [.1,0,.1,.3],\n",
    "                  [.2,.1,0,.2],\n",
    "                  [.3,.3,.2,0]],      dtype=np.float64)\n",
    "    \n",
    "    h = np.array([1, 2, 1, 2],        dtype=np.float64)\n",
    "\n",
    "    check_parameters(w,J,h)\n",
    "\n",
    "    #intialize the optimal learning parameters for 4-qubits\n",
    "    learning_rate_qbm = .1\n",
    "    maxiter_qbm = 2**11\n",
    "\n",
    "    learning_rate_bm = .004\n",
    "    maxiter_bm = 2**22\n",
    "\n",
    "    # map forward and back\n",
    "    w_qm = forward_mapping(w, J, h, learning_rate_qbm, maxiter_qbm, plot_convergence = True, perform_checks = True)\n",
    "    w_recovered, J_recovered, h_recovered = inverse_mapping(w_qm, learning_rate_bm, maxiter_bm, plot_convergence = True, perform_checks = False)\n",
    "\n",
    "    print('Difference in initial and recovered parameters:')\n",
    "    print(f'w: \\n {np.abs(w - w_recovered)}')\n",
    "    print(f'J: \\n {np.abs(J - J_recovered)}')\n",
    "    print(f'h: \\n {np.abs(h - h_recovered)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2f4a628",
   "metadata": {},
   "source": [
    "## Forward Mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f60d7761",
   "metadata": {},
   "source": [
    "We have to make sure that for all values of the parameter sweep, we don't need to use min_w. Probably write another function for that sometime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9704ffa",
   "metadata": {},
   "source": [
    "### Results 2-qubits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2828ee4e",
   "metadata": {},
   "source": [
    "#### Varying J in Homogenous system with biases and self-interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classical parameters\n",
    "N = 2\n",
    "w = np.array([[3, 3], \n",
    "              [3, 3]],   dtype=np.float64)\n",
    "J = np.array([[0, 1],\n",
    "              [1, 0]],   dtype=np.float64)\n",
    "h = np.array([.1, .1],   dtype=np.float64)\n",
    "\n",
    "# define the classical parameter that will be varied\n",
    "param = 'J'\n",
    "index = (0, 1)\n",
    "\n",
    "# define the range over which the classical parameter will be varied\n",
    "start = -2.15\n",
    "end = 2.15\n",
    "steps = 100\n",
    "\n",
    "# intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_qbm = .5\n",
    "maxiter_qbm = 2**22\n",
    "\n",
    "#only relevant for checking if the mapping is inverisble\n",
    "learning_rate_bm = .8\n",
    "maxiter_bm = 2**20\n",
    "\n",
    "# specify the quantum parameters to plot, if you select none, all that change more than the threshold over the sweep will be plotted\n",
    "qm_params_to_plot = None\n",
    "# plot_params = [ 'J^{XZ}_{14}', 'h^{Y}_{2}', ''sigma^{YXZ}_{123}] #should work somehthing like this\n",
    "\n",
    "# specify the change threshold\n",
    "threshold = 0.1\n",
    "\n",
    "# call the function\n",
    "quantum_params = analyze_forward_parameter_mapping(start, end, steps, param, index, w, J, h, learning_rate_qbm, maxiter_qbm, learning_rate_bm, maxiter_bm)\n",
    "\n",
    "plot_forward_mapping(quantum_params, start, end, steps, param, index, N, qm_params_to_plot, threshold)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5a12bb9",
   "metadata": {},
   "source": [
    "#### Varying J in Homogenous system with self-interactions but no biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "N = 2\n",
    "w = np.array([[3, 3], \n",
    "              [3, 3]], dtype=np.float64)\n",
    "J = np.array([[0, 1],\n",
    "              [1, 0]],   dtype=np.float64)\n",
    "h = np.array([.00000000000001, .00000000000001],    dtype=np.float64)\n",
    "\n",
    "check_parameters(w, J, h)\n",
    "\n",
    "# define the classical parameter that will be varied\n",
    "param = 'J'\n",
    "index = (0, 1)\n",
    "\n",
    "# define the range over which the classical parameter will be varied\n",
    "start = -2.15\n",
    "end = 2.15\n",
    "steps = 100\n",
    "\n",
    "# intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_qbm = .5\n",
    "maxiter_qbm = 2**11\n",
    "\n",
    "#only relevant for checking if the mapping is inverisble\n",
    "learning_rate_bm = .8\n",
    "maxiter_bm = 2**20\n",
    "\n",
    "# specify the quantum parameters to plot, if you select none, all that change more than the threshold over the sweep will be plotted\n",
    "qm_params_to_plot = None\n",
    "# plot_params = [ 'J^{XZ}_{14}', 'h^{Y}_{2}', ''sigma^{YXZ}_{123}] #should work somehthing like this\n",
    "\n",
    "# specify the change threshold\n",
    "threshold = 0.1\n",
    "\n",
    "# call the function\n",
    "quantum_params = analyze_forward_parameter_mapping(start, end, steps, param, index, w, J, h, learning_rate_qbm, maxiter_qbm, learning_rate_bm, maxiter_bm)\n",
    "\n",
    "plot_forward_mapping(quantum_params, start, end, steps, param, index, N, qm_params_to_plot, threshold)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c916c38",
   "metadata": {},
   "source": [
    "### Results 3-qubit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b94269a",
   "metadata": {},
   "source": [
    "#### Varying J in Homogenous system with biases and self-interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69dc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "N = 3\n",
    "w = np.array([[3, 3 ,3],\n",
    "              [3, 3, 3],\n",
    "              [3, 3, 3]], dtype = np.float64)\n",
    "\n",
    "J = np.array([[0, 0, 0],\n",
    "              [0 ,0, 0],\n",
    "              [0, 0, 0]], dtype = np.float64)\n",
    "\n",
    "h = np.array([.1, .1, .1],   dtype = np.float64)\n",
    "\n",
    "#check if we got valid parameters\n",
    "check_parameters(w, J, h)\n",
    "\n",
    "# define the classical parameter that will be varied\n",
    "param = 'J'\n",
    "index = (0, 1)\n",
    "\n",
    "# define the range over which the classical parameter will be varied\n",
    "start = -.7\n",
    "end = .7\n",
    "steps = 30\n",
    "\n",
    "# intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_qbm = .5\n",
    "maxiter_qbm = 2**11\n",
    "\n",
    "#only relevant for checking if the mapping is inverisble\n",
    "learning_rate_bm = .8\n",
    "maxiter_bm = 2**20\n",
    "\n",
    "# specify the quantum parameters to plot, if you select none, all that change more than the threshold over the sweep will be plotted\n",
    "qm_params_to_plot = ['$J_{12}^{ZZ}$', '$J_{12}^{YY}$'] \n",
    "\n",
    "\n",
    "# specify the change threshold\n",
    "threshold = 0.00001\n",
    "\n",
    "# call the function\n",
    "quantum_params = analyze_forward_parameter_mapping(start, end, steps, param, index, w, J, h, learning_rate_qbm, maxiter_qbm, learning_rate_bm, maxiter_bm)\n",
    "\n",
    "plot_forward_mapping(quantum_params, start, end, steps, param, index, N, qm_params_to_plot, threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91180bd8",
   "metadata": {},
   "source": [
    "Smaller biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae32a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "N = 3\n",
    "w = np.array([[3, 3 ,3],\n",
    "              [3, 3, 3],\n",
    "              [3, 3, 3]], dtype = np.float64)\n",
    "\n",
    "J = np.array([[0, 0, 0],\n",
    "              [0 ,0, 0],\n",
    "              [0, 0, 0]], dtype = np.float64)\n",
    "\n",
    "h = np.array([.000001, .000001, .000001],   dtype = np.float64)\n",
    "\n",
    "#check if we got valid parameters\n",
    "check_parameters(w, J, h)\n",
    "\n",
    "# define the classical parameter that will be varied\n",
    "param = 'J'\n",
    "index = (1, 2)\n",
    "\n",
    "# define the range over which the classical parameter will be varied\n",
    "start = -.7\n",
    "end = .7\n",
    "steps = 30\n",
    "\n",
    "# intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_qbm = .5\n",
    "maxiter_qbm = 2**11\n",
    "\n",
    "#only relevant for checking if the mapping is inverisble\n",
    "learning_rate_bm = .8\n",
    "maxiter_bm = 2**20\n",
    "\n",
    "# specify the quantum parameters to plot, if you select none, all that change more than the threshold over the sweep will be plotted\n",
    "qm_params_to_plot = ['$J_{12}^{ZZ}$', '$J_{12}^{YY}$'] \n",
    "\n",
    "# specify the change threshold\n",
    "threshold = 0.00001\n",
    "\n",
    "# call the function\n",
    "quantum_params = analyze_forward_parameter_mapping(start, end, steps, param, index, w, J, h, learning_rate_qbm, maxiter_qbm, learning_rate_bm, maxiter_bm)\n",
    "\n",
    "plot_forward_mapping(quantum_params, start, end, steps, param, index, N, qm_params_to_plot, threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ca54524",
   "metadata": {},
   "source": [
    "### Same shit, larger biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd4c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "N = 3\n",
    "w = np.array([[3, 3 ,3],\n",
    "              [3, 3, 3],\n",
    "              [3, 3, 3]], dtype = np.float64)\n",
    "\n",
    "J = np.array([[0, 0, 0],\n",
    "              [0 ,0, 0],\n",
    "              [0, 0, 0]], dtype = np.float64)\n",
    "\n",
    "h = np.array([.75, .75, .75],   dtype = np.float64)\n",
    "\n",
    "#check if we got valid parameters\n",
    "check_parameters(w, J, h)\n",
    "\n",
    "# define the classical parameter that will be varied\n",
    "param = 'J'\n",
    "index = (0, 1)\n",
    "\n",
    "# define the range over which the classical parameter will be varied\n",
    "start = 0\n",
    "end = .70\n",
    "steps = 30\n",
    "\n",
    "learning_rate_qbm = .5\n",
    "maxiter_qbm = 2**20\n",
    "\n",
    "#only relevant for checking if the mapping is inversible\n",
    "learning_rate_bm = .8\n",
    "maxiter_bm = 2**20\n",
    "\n",
    "# specify the quantum parameters to plot, if you select none, all that change more than the threshold over the sweep will be plotted\n",
    "qm_params_to_plot = ['$J_{12}^{ZZ}$', '$J_{12}^{YY}$'] \n",
    "\n",
    "# specify the change threshold\n",
    "threshold = 0.01\n",
    "\n",
    "# call the function\n",
    "quantum_params = analyze_forward_parameter_mapping(start, end, steps, param, index, w, J, h, learning_rate_qbm, maxiter_qbm, learning_rate_bm, maxiter_bm)\n",
    "\n",
    "plot_forward_mapping(quantum_params, start, end, steps, param, index, N, qm_params_to_plot, threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5205472",
   "metadata": {},
   "source": [
    "### Results 4-qubit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7176de76",
   "metadata": {},
   "source": [
    "Expect 30-40 min for convergence of QBM. Try agressive learning rates first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_run = 1  #skip this due to long computation time\n",
    "if dont_run == 0: \n",
    "\n",
    "    # initialize the classical parameters\n",
    "    w = np.array([[4.,2.,4.,5.],\n",
    "                [2.,4.,1.,3.],\n",
    "                [4.,1.,2.,4.],\n",
    "                [5.,3.,4.,5.]])\n",
    "    J = np.array([[0,1,2,3],\n",
    "                [1,0,1,3],\n",
    "                [2,1,0,2],\n",
    "                [3,3,2,0]])\n",
    "    h = np.array([1, 2, 3, 4])\n",
    "\n",
    "    check_parameters(w,J,h)\n",
    "\n",
    "    # define the start, end and step size for the parameter change\n",
    "    start = -10\n",
    "    end = 10\n",
    "    steps = 30\n",
    "\n",
    "    # define the parameter and index you want to change\n",
    "    param = 'J'\n",
    "    index = (0, 1)\n",
    "\n",
    "    # intialize the optimal learning parameters for 3-qubits\n",
    "    learning_rate_qbm = .9\n",
    "    maxiter_qbm = 2**15\n",
    "\n",
    "    learning_rate_bm = .9\n",
    "    maxiter_bm = 2**20\n",
    "\n",
    "    # specify the quantum parameters to plot, if you select none, all that change more than the threshold over the sweep will be plotted\n",
    "    plot_params = None\n",
    "    # plot_params = ['w_0_1', 'J_0_1', 'h_0']\n",
    "\n",
    "    # specify the change threshold\n",
    "    threshold = 0.01\n",
    "\n",
    "    # call the function\n",
    "    analyze_forward_parameter_mapping(start, end, steps, param, index, w, J, h, learning_rate_qbm, maxiter_qbm, learning_rate_bm, maxiter_bm, plot_params, threshold)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d83517b",
   "metadata": {},
   "source": [
    "## Inverse Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the dynamics\n",
    "N = 2\n",
    "steps = 200\n",
    "W = compute_transition_matrix(w, h, J)\n",
    "trajectory = simulate_dynamics(W, steps, N)\n",
    "plot_combined_dynamics(trajectory, N)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "904146ed",
   "metadata": {},
   "source": [
    "### Inverse mapping reversibility\n",
    "\n",
    "- Check what happens when you use 3rd body interactions in the QM hamiltonian"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa36f97f",
   "metadata": {},
   "source": [
    "#### When QM parameters map to a dynamics that does not satisfy detailed balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77653b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "\n",
    "#initialize the quantum paramters\n",
    "interaction_matrices,interaction_labels,interaction_weights = generate_interaction_matrices(N)\n",
    "\n",
    "#Leave out the terms containing odd amounts of Y\n",
    "interaction_weights[('I', 'I')] =  0\n",
    "\n",
    "#hx1 > Jxz\n",
    "interaction_weights[('I', 'X')] = .8   #hx1\n",
    "interaction_weights[('X', 'Z')] = .5   #Jxz\n",
    "\n",
    "#hx2 > Jzx\n",
    "interaction_weights[('X', 'I')] = .6   #hx2\n",
    "interaction_weights[('Z', 'X')] = .4   #Jzx\n",
    "\n",
    "# Jxx > Jyy\n",
    "interaction_weights[('X', 'X')] = .8   #Jxx\n",
    "interaction_weights[('Y', 'Y')] = .2   #Jyy\n",
    "\n",
    "# all Z-only terms < 1\n",
    "interaction_weights[('I', 'Z')] = .1   #hz1\n",
    "interaction_weights[('Z', 'I')] = .1   #hz2\n",
    "interaction_weights[('Z', 'Z')] = .4   #Jzz\n",
    "\n",
    "# make sure there are no imaginary components\n",
    "interaction_weights = set_odd_y_interactions_to_zero(interaction_labels, interaction_weights)\n",
    "\n",
    "#generate a rho and its associated W\n",
    "w_qm = weights_dict_to_array(interaction_labels, interaction_weights) \n",
    "\n",
    "rho = rho_model(w_qm,interaction_matrices)\n",
    "check_density_matrix(rho)\n",
    "\n",
    "W = density_matrix_to_transition_matrix(rho)\n",
    "test_transition_matrix(W)\n",
    "check_detailed_balance(W)\n",
    "\n",
    "#infer the classical parameters using a classical boltzmann machine\n",
    "w, J, h = infer_parameters(W, maxiter = 2**14, learning_rate = .2, plot = True)\n",
    "\n",
    "W_return = compute_transition_matrix(w, h, J)\n",
    "\n",
    "print(f'Initial qm parameters: {w_qm}')\n",
    "print(f'Density matrix: \\n {rho_model(w_qm,interaction_matrices)}')\n",
    "print(f'Transition matrix: \\n {W}')\n",
    "print(f'Returned transition matrix \\n {W_return}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array( [[0.49, 0.21, 0.22, 0.08],\n",
    "               [0.46, 0.24, 0.23, 0.07],\n",
    "               [0.46, 0.22, 0.24, 0.08],\n",
    "               [0.44, 0.18, 0.21, 0.17]])\n",
    "\n",
    "check_detailed_balance(W)\n",
    "\n",
    "# W_sym = W + \n",
    "\n",
    "#check validity of W\n",
    "check_detailed_balance(W)\n",
    "test_transition_matrix(W)\n",
    "\n",
    "w, J, h = infer_parameters(W)\n",
    "\n",
    "print(w)\n",
    "print(J)\n",
    "print(h)\n",
    "\n",
    "W_return = compute_transition_matrix(w, h, J)\n",
    "print(W_return)\n",
    "check_parameters(w,J,h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79386eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize the learning parameters\n",
    "learning_rate_qbm = .9\n",
    "maxiter_qbm = 2**14\n",
    "\n",
    "learning_rate_bm = .9\n",
    "maxiter_bm = 2**18\n",
    "\n",
    "\n",
    "w, J, h = inverse_mapping(w_qm, learning_rate_bm, maxiter_bm, plot_convergence = True)\n",
    "\n",
    "print('Rho maps to classical parameters:')\n",
    "print(f' w: \\n {w}')\n",
    "print(f' J: \\n {J}')\n",
    "print(f' h: \\n {h}')\n",
    "\n",
    "w_qm_recovered = forward_mapping(w, J, h, learning_rate_qbm, maxiter_qbm, plot_convergence = True)\n",
    "\n",
    "print(f'Difference in initial and recovered parameters: \\n {np.abs(w_qm - w_qm_recovered)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c38e8b31",
   "metadata": {},
   "source": [
    "Splitting W into a symmetric and assymetric part. Maybe we can split into a symmetric part and assymetric part and individually see to which QM parameters these transition matrices map back. In the symmetric case, we can also map it forward to classical parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc7f170d",
   "metadata": {},
   "source": [
    "#### check if symmetry of the xz zx terms leads to detailed balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the weights using the dictionary\n",
    "num_qubits = 2\n",
    "interaction_matrices,interaction_labels,interaction_weights = generate_interaction_matrices(num_qubits)\n",
    "\n",
    "#Leave out the terms containing odd amounts of Y\n",
    "interaction_weights[('I', 'I')] =  0\n",
    "\n",
    "#hx1 > Jxz\n",
    "interaction_weights[('I', 'X')] = .8   #hx1\n",
    "interaction_weights[('X', 'Z')] = .5   #Jxz\n",
    "\n",
    "#hx2 > Jzx\n",
    "interaction_weights[('X', 'I')] = .6   #hx2\n",
    "interaction_weights[('Z', 'X')] = .4   #Jzx\n",
    "\n",
    "# Jxx > Jyy\n",
    "interaction_weights[('X', 'X')] = .6   #Jxx\n",
    "interaction_weights[('Y', 'Y')] = .2   #Jyy\n",
    "\n",
    "# all Z-only terms < 1\n",
    "interaction_weights[('I', 'Z')] = .1   #hz1\n",
    "interaction_weights[('Z', 'I')] = .1   #hz2\n",
    "interaction_weights[('Z', 'Z')] = .4   #Jzz\n",
    "\n",
    "# make sure there are no imaginary components\n",
    "interaction_weights = set_odd_y_interactions_to_zero(interaction_labels, interaction_weights)\n",
    "\n",
    "\n",
    "#qm hamiltonian to density matrix\n",
    "w_qm = weights_dict_to_array(interaction_labels, interaction_weights) \n",
    "rho = rho_model(w_qm, interaction_matrices)\n",
    "\n",
    "print('Input QM parameters')\n",
    "for k, v in interaction_weights.items():\n",
    "    print (k, ':', v)\n",
    "print(f' Associated density matrix: \\n {np.real(rho)}')\n",
    "print(f' Associated eigenvalues of rho {np.real(np.linalg.eigvals(rho))}')\n",
    "#density matrix to transition matrix\n",
    "W = density_matrix_to_transition_matrix(rho)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edd4bf5a",
   "metadata": {},
   "source": [
    "### 2-Qubits Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c04ab93",
   "metadata": {},
   "source": [
    "#### Influence of Jxx term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the qm parameters using the weight dictionary\n",
    "N = 2\n",
    "interaction_matrices,interaction_labels,interaction_weights = generate_interaction_matrices(N)\n",
    "#set everything to zero\n",
    "w_qm = weights_dict_to_array(interaction_labels, interaction_weights) \n",
    "\n",
    "# define the quantum parameter that will be varied\n",
    "quantum_param_label = ('X','X')\n",
    "\n",
    "# define the range over which the classical parameter will be varied\n",
    "start = -2\n",
    "end = 2\n",
    "steps = 50\n",
    "\n",
    "# intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_bm = .9\n",
    "maxiter_bm = 2**19\n",
    "\n",
    "# specify the classical parameters to plot, if you select none, all that change more than the threshold over the sweep will be plotted\n",
    "classical_params_to_plot = None\n",
    "# plot_params = ['w_{12}', 'J_{14}', 'h_{2}'] #should work somehthing like this\n",
    "\n",
    "# specify the change threshold\n",
    "threshold = 0.01\n",
    "\n",
    "# call the function\n",
    "analyze_inverse_parameter_mapping(start, end, steps, quantum_param_label, w_qm, learning_rate_bm,  maxiter_bm, classical_params_to_plot = None, threshold=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f695aae",
   "metadata": {},
   "source": [
    "#### Influence of Jyy term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the qm parameters using the weight dictionary\n",
    "N = 2\n",
    "interaction_matrices,interaction_labels,interaction_weights = generate_interaction_matrices(N)\n",
    "#set everything to zero\n",
    "w_qm = weights_dict_to_array(interaction_labels, interaction_weights) \n",
    "\n",
    "# define the quantum parameter that will be varied\n",
    "quantum_param_label = ('Y','Y')\n",
    "\n",
    "# define the range over which the classical parameter will be varied\n",
    "start = -2\n",
    "end = 2\n",
    "steps = 50\n",
    "# intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_bm = .9\n",
    "maxiter_bm = 2**19\n",
    "\n",
    "# specify the classical parameters to plot, if you select none, all that change more than the threshold over the sweep will be plotted\n",
    "classical_params_to_plot = None\n",
    "\n",
    "# specify the change threshold\n",
    "threshold = 0.01\n",
    "\n",
    "# call the function\n",
    "analyze_inverse_parameter_mapping(start, end, steps, quantum_param_label, w_qm, learning_rate_bm,  maxiter_bm, classical_params_to_plot = None, threshold=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aefc8640",
   "metadata": {},
   "source": [
    "#### Influence of Jzz term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df71769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the qm parameters using the weight dictionary\n",
    "N = 2\n",
    "interaction_matrices,interaction_labels,interaction_weights = generate_interaction_matrices(N)\n",
    "#set everything to zero\n",
    "w_qm = weights_dict_to_array(interaction_labels, interaction_weights) \n",
    "\n",
    "# define the quantum parameter that will be varied\n",
    "quantum_param_label = ('Z','Z')\n",
    "\n",
    "# define the range over which the classical parameter will be varied\n",
    "start = -2\n",
    "end = 2\n",
    "steps = 50\n",
    "\n",
    "# intialize the optimal learning parameters for 2-qubits\n",
    "learning_rate_bm = .9\n",
    "maxiter_bm = 2**19\n",
    "\n",
    "# specify the classical parameters to plot, if you select none, all that change more than the threshold over the sweep will be plotted\n",
    "classical_params_to_plot = None\n",
    "\n",
    "# specify the change threshold\n",
    "threshold = 0.01\n",
    "\n",
    "# call the function\n",
    "analyze_inverse_parameter_mapping(start, end, steps, quantum_param_label, w_qm, learning_rate_bm,  maxiter_bm, classical_params_to_plot = None, threshold=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a02f6b11",
   "metadata": {},
   "source": [
    "### 3 Qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 3\n",
    "interaction_matrices,interaction_labels,interaction_weights = generate_interaction_matrices(num_qubits)\n",
    "\n",
    "#Leave out the terms containing odd amounts of Y\n",
    "interaction_weights[('I', 'I', 'I')] = 0    #log(Z)\n",
    "\n",
    "#hxi > Jxzij + jxzik\n",
    "interaction_weights[('X', 'I', 'I')] = .5   #hx1\n",
    "interaction_weights[('X', 'Z', 'I')] = .1   #Jxz12\n",
    "interaction_weights[('X', 'I', 'Z')] = .2   #Jxz13\n",
    "\n",
    "interaction_weights[('I', 'X', 'I')] = .4   #hx2\n",
    "interaction_weights[('I', 'X', 'Z')] = .1   #Jxz23\n",
    "interaction_weights[('Z', 'X', 'I')] = .1   #Jzx12\n",
    "\n",
    "interaction_weights[('I', 'I', 'X')] = .8   #hx3\n",
    "interaction_weights[('I', 'Z', 'X')] = .2   #Jzx23\n",
    "interaction_weights[('Z', 'I', 'X')] = .3   #Jzx13\n",
    "\n",
    "\n",
    "# Jxx > Jyy\n",
    "interaction_weights[('X', 'X', 'I')] = .6   #Jxx12\n",
    "interaction_weights[('Y', 'Y', 'I')] = .4   #Jyy12\n",
    "\n",
    "interaction_weights[('I', 'X', 'X')] = .7   #Jxx23\n",
    "interaction_weights[('I', 'Y', 'Y')] = .4   #Jyy23\n",
    "\n",
    "interaction_weights[('X', 'I', 'X')] = .8   #Jxx13\n",
    "interaction_weights[('Y', 'I', 'Y')] = .4   #Jyy13\n",
    "\n",
    "\n",
    "# all Z-only terms < 1\n",
    "interaction_weights[('Z', 'I', 'I')] = .1   #hz1\n",
    "interaction_weights[('I', 'Z', 'I')] = .1   #hz2\n",
    "interaction_weights[('I', 'I', 'Z')] = .1   #hz3\n",
    "\n",
    "interaction_weights[('Z', 'Z', 'I')] = .3   #Jzz12\n",
    "interaction_weights[('Z', 'I', 'Z')] = .3   #Jzz13\n",
    "interaction_weights[('I', 'Z', 'Z')] = .3   #Jzz23\n",
    "\n",
    "# make sure there are no imaginary components\n",
    "interaction_weights = set_odd_y_interactions_to_zero(interaction_labels, interaction_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
